

#   数据挖掘

[TOC]



## Jupyter Notebook

Jupyter项目是一个非盈利的开源项目，源于2014年的ipython项目，并逐渐发展为支持跨所有编程语言的交互式数据科学计算工具

- 是ipython的加强网页版，是一个开源的web程序
- 名字源有Julia，Python和R
- 是一款程序员和科学工作者的编辑、文档、笔记，展示软件
- .ipynb文件格式是用于计算型叙述的JSON文档格式的正式规范

为什么使用Jupyter？

- 画图方便的优势
- 数据展示方便的优势

### 快速上手Jupyter Notebook

界面启动 创建文件

在cmd窗口输入

```shell
jupyter notebook
```

新建notebook的文档格式是.ipynb

![image-20210905100022783](.\pic\image-20210905100022783.png)

![image-20210905095722397](.\pic\image-20210905095722397.png)

### cell操作

什么是cell？

cell：一对In Out会话被视为一个代码单元，称为cell

**Jupyter支持两种模式：**

- 编辑模式（Enter）

    命令模式下 `回车Enter` 或者 `鼠标点击` cell进入编辑模式

    可以操作cell内文本代码，剪切/复制/粘贴移动等操作

- 命令模式（Esc）

    按`Esc`退出编辑，进入命令模式

    可以`操作cell单元`本身进行剪切/复制/粘贴/移动等操作

**常用的快捷键：**

`Shift+Enter`	执行本单元代码，并跳转到下一单元

`Ctrl+Enter`	执行本单元代码，留在本单元

命令模式下：`A`	`B`	`双击D`

**markdown**

`# 一级标题`

`- 缩进`



## Matplotlib画图

什么是Matplotlib？

- 专门用于开发2D图表（包括3D图表）
- 使用起来及其简单
- 以渐进，交互式方式实现数据可视化

为什么学习Matplotlib ---画图

数据可视化，可视化是整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法

### 实现一个简单的Matplotlib画图

```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure() # 创建一块画布
plt.plot([1, 0, 9], [4, 5, 6])
plt.show()
```

![image-20210905110614554](.\pic\image-20210905110614554.png)

### Matplotlib三层结构	

#### **容器层**

**画板层**(Canvas) ---**画布层**(Figure)plt.figure() ---**绘图区/坐标系**(axes)plt.subplots()

#### 辅助显示层

辅助显示层为Axes内除了根据数据绘制出来的图像以外的内容，该层的设置可以使图像显示更加直观更加容易被用户理解，但又不会对图像产生实际的影响

#### 图像层

图像层指的是通过plot，scatter，bar，histogram，pie等函数根据数据绘制出的图像

### 折线图plot与基础绘图功能

#### matplotlib.pyplot模块

matplotlib.pyplot包含了一系列类似于matplotlib的画图函数，它的函数作用于当前图形的当前坐标系

```python
import matplotlib.pyplot as plt
```

#### 折线图绘制与显示

```python
import matplotlib.pyplot as plt

# 创建画布
plt.figure()
# 绘制折线图
plt.plot([1, 2, 3, 4, 5, 6, 7], [17, 17, 18, 14, 11, 11, 13])
# 显示图像
plt.show()
```

![image-20210905113654039](.\pic\image-20210905113654039.png)

#### 设置画布属性与图片保存

```
plt.figure(figsize=(), dpi=)
	figsize:指定图的长宽
	dpi:图像的清洗度
	返回fig对象
plt.savefig(path)
```

```python
import matplotlib.pyplot as plt

# 创建画布
plt.figure(figsize=(20, 8), dpi=120)
# 绘制折线图
plt.plot([1, 2, 3, 4, 5, 6, 7], [17, 17, 18, 14, 11, 11, 13])

# 保存图像( 如果该行代码写到plt.show()后保存的图片是空白 )
plt.savefig("test.png")

# 显示图像
plt.show() # 会释放figure资源
```

![test](.\pic\test.png)

#### 完善原始折线图(辅助显示层)

```python
import matplotlib.pyplot as plt
import random

# 1 准备数据 x y
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]

plt.figure(figsize=(20, 8), dpi=80)
plt.plot(x, y_shanghai)
plt.show()
```

![image-20210905115612134](.\pic\image-20210905115612134.png)

##### 中文问题解决

https://blog.csdn.net/gdhenry92/article/details/102930238

但是我不管用，所有找来以下解决方法

在代码中添加以下代码

```python
plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号
```

##### 修改x轴y轴刻度

```python
import matplotlib.pyplot as plt
import random

plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号
             
# 1 准备数据 x y
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]

plt.figure(figsize=(20, 8), dpi=80)
plt.plot(x, y_shanghai)


# 修改x y刻度
x_label = ["11点{}分".format(i) for i in x] # 准备x的刻度说明a

plt.xticks(x[::5], x_label[::5])
plt.yticks(range(0, 40, 5))

plt.show()
```

![image-20210905160255509](.\pic\image-20210905160255509.png)

##### 添加网格

```python
plt.grid(True, linestyle='--', alpha=0.5) # 默认是true, 网格风格， 透明度
```

![image-20210905161514297](.\pic\image-20210905161514297.png)

##### 添加描述信息

```python
plt.xlabel("时间")
plt.ylabel("温度")
plt.title("中午11点0分到12点之间的温度变化图示")
```

```python
import matplotlib.pyplot as plt
import random

plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号
             
# 1 准备数据 x y
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]

plt.figure(figsize=(20, 8), dpi=80)
plt.plot(x, y_shanghai)


# 修改x y刻度
x_label = ["11点{}分".format(i) for i in x] # 准备x的刻度说明a

plt.xticks(x[::5], x_label[::5])
plt.yticks(range(0, 40, 5))

plt.grid(True, linestyle='--', alpha=0.5) # 默认是true, 网格风格， 透明度

plt.xlabel("时间")
plt.ylabel("温度")
plt.title("中午11点0分到12点之间的温度变化图示")


plt.show()
```

![image-20210905161819192](.\pic\image-20210905161819192.png)

#### 完善原始折线图(图像层)

##### 多个plot

```python
import matplotlib.pyplot as plt
import random

plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号
             
# 1 准备数据 x y
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]

plt.figure(figsize=(20, 8), dpi=80)
plt.plot(x, y_shanghai, color="r", label="上海")

# 准备另一个城市的数据
y_beijin = [random.uniform(1, 3) for i in x]
plt.plot(x, y_beijin, color="b", linestyle="-.", label="北京")

# 修改x y刻度
x_label = ["11点{}分".format(i) for i in x] # 准备x的刻度说明a

plt.xticks(x[::5], x_label[::5])
plt.yticks(range(0, 40, 5))


# 添加辅助显示层的样式
plt.grid(True, linestyle='--', alpha=0.5) # 默认是true, 网格风格， 透明度

plt.xlabel("时间")
plt.ylabel("温度")
plt.title("上海，北京中午11点0分到12点之间的温度变化图示")
# 显示图例
plt.legend(loc=0) # 修改显示的位置

plt.show()
```

![image-20210905163015887](.\pic\image-20210905163015887.png)

##### 多个坐标系显示-plt.subplots(面向对象的画图方法)

```python
figure, axes = plt.subplots(nrows=1, ncols=2, **fig_kw)
axes[0].方法名
axes[1].方法名
```

```python
import matplotlib.pyplot as plt
import random

plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号

x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]
y_beijing = [random.uniform(1, 4) for i in x]

# 创建画布
figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(20, 8), dpi = 80)

# 绘制图像
axes[0].plot(x, y_shanghai, color="r", linestyle="--", label="上海")
axes[1].plot(x, y_beijing, color="b", linestyle="-.", label="北京")

# 显示图例
axes[0].legend()
axes[1].legend()

# 刻度修改
x_label = ["11点{}分".format(i) for i in x] # 准备x的刻度说明a
axes[0].set_xticks(x[::5])
axes[0].set_xticklabels(x_label[::5])
axes[0].set_yticks(range(0, 40, 5))

axes[1].set_xticks(x[::5])
axes[1].set_xticklabels(x_label[::5])
axes[1].set_yticks(range(0, 40, 5))

# 添加网格显示
axes[0].grid(linestyle="--", alpha=0.5)
axes[1].grid(linestyle="--", alpha=0.5)

# 添加描述信息
axes[0].set_xlabel("时间")
axes[0].set_ylabel("温度")
axes[0].set_title("上海中午11点0分到12点之间的温度变化图示")
axes[1].set_xlabel("时间")
axes[1].set_ylabel("温度")
axes[1].set_title("北京中午11点0分到12点之间的温度变化图示")

plt.show()
```

![image-20210905170009965](.\pic\image-20210905170009965.png)

#### 其他图像

```python
import matplotlib.pyplot as plt
import numpy as np 

x = np.linspace(-10, 10, 1000)
y = np.sin(x)

plt.figure(figsize=(40, 8), dpi = 100)

plt.plot(x, y)
plt.grid()

plt.show()
```

![image-20210905170744698](.\pic\image-20210905170744698.png)

```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-1, 1, 1000) # 左闭右闭
y = 2 * x * x

plt.figure(figsize=(40, 8), dpi = 100)

plt.plot(x, y)
plt.grid()

plt.show()
```

![image-20210905171218283](.\pic\image-20210905171218283.png)

### 散点图

```python
import matplotlib.pyplot as plt

x = [1, 2, 3, 4, 5, 6, 7]
y = [2, 3, 4, 7, 7, 8, 8]

plt.figure(figsize=(20, 8), dpi=100)

plt.scatter(x, y)

plt.show()
```

![image-20210905172340123](.\pic\image-20210905172340123.png)

### 柱状图

```python
import matplotlib.pyplot as plt

# 准备数据
movice_name = ['雷神3：诸神黄昏','正义联盟','东方快车谋杀案','寻梦环游记','全球风暴', '降魔传','追捕','七十七天','密战','狂兽','其它']
tickets = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]

# 创建画布
plt.figure(figsize=(20, 8), dpi = 100)

# 绘制柱状图
x_ticks = range(len(movice_name))
plt.bar(x_ticks, tickets, color=['b', 'r', 'g', 'y', 'c', 'm', 'y', 'k', 'c', 'g', 'b'])  # 有几个类别， y

# 修改x的刻度
plt.xticks(x_ticks, movice_name)


# 添加标题
plt.title("电影票房收入对比")

# 添加网格显示
plt.grid(linestyle="--", alpha=0.5)

# 显示图像
plt.show()
```

![image-20210905174649434](.\pic\image-20210905174649434.png)

```python
import matplotlib.pyplot as plt
movice_name = ['雷神3：诸神黄昏', '正义联盟', '寻梦环游记']
first_day = [10587.6, 10062.5, 1275.7]
first_weekend = [36224.9, 34479, 11830]

# 创建画布
plt.figure(figsize=(20, 8), dpi = 100)

plt.bar(range(3), first_day,width = 0.2, label = "首日票房")
plt.bar([0.2, 1.2, 2.2], first_weekend, width = 0.2,  label = "首周票房")

# 修改刻度
plt.xticks([0.1, 1.1, 2.1], movice_name)

# 显示图例
plt.legend()

plt.show()
```

![image-20210905191901060](.\pic\image-20210905191901060.png)

### 直方图

```python
import matplotlib.pyplot as plt

time = [131,  98, 125, 131, 124, 139, 131, 117, 128, 108, 135, 138, 131, 102, 107, 114, 119, 128, 121, 142, 127, 130, 124, 101, 110, 116, 117, 110, 128, 128, 115,  99, 136, 126, 134,  95, 138, 117, 111,78, 132, 124, 113, 150, 110, 117,  86,  95, 144, 105, 126, 130,126, 130, 126, 116, 123, 106, 112, 138, 123,  86, 101,  99, 136,123, 117, 119, 105, 137, 123, 128, 125, 104, 109, 134, 125, 127,105, 120, 107, 129, 116, 108, 132, 103, 136, 118, 102, 120, 114,105, 115, 132, 145, 119, 121, 112, 139, 125, 138, 109, 132, 134,156, 106, 117, 127, 144, 139, 139, 119, 140,  83, 110, 102,123,107, 143, 115, 136, 118, 139, 123, 112, 118, 125, 109, 119, 133,112, 114, 122, 109, 106, 123, 116, 131, 127, 115, 118, 112, 135,115, 146, 137, 116, 103, 144,  83, 123, 111, 110, 111, 100, 154,136, 100, 118, 119, 133, 134, 106, 129, 126, 110, 111, 109, 141,120, 117, 106, 149, 122, 122, 110, 118, 127, 121, 114, 125, 126,114, 140, 103, 130, 141, 117, 106, 114, 121, 114, 133, 137,  92,121, 112, 146,  97, 137, 105,  98, 117, 112,  81,  97, 139, 113,134, 106, 144, 110, 137, 137, 111, 104, 117, 100, 111, 101, 110,105, 129, 137, 112, 120, 113, 133, 112,  83,  94, 146, 133, 101,131, 116, 111,  84, 137, 115, 122, 106, 144, 109, 123, 116, 111,111, 133, 150]

# 创建画布
plt.figure(figsize = (20, 8), dpi = 80)

# 创建直方图
distance = 2 # 柱距
group_num = (max(time) - min(time)) // distance # 柱数
plt.hist(time, bins = group_num, density = True)

# 修改x轴刻度
plt.xticks(range(min(time), max(time)+2, distance))

# 添加网格
plt.grid(linestyle = "-.", alpha = 0.5)

# 添加标题
plt.title("******")
# 显示图像
plt.show()
```

![image-20210905201137397](.\pic\image-20210905201137397.png)

### 饼图

```python
import matplotlib.pyplot as plt

# 准备数据
movice_name = ['雷神3：诸神黄昏','正义联盟','东方快车谋杀案','寻梦环游记','全球风暴', '降魔传','追捕','七十七天','密战','狂兽','其它']
place_count = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]

# 创建画布
plt.figure(figsize = (20, 8), dpi = 80)

# 绘制饼图
plt.pie(place_count, labels=movice_name, colors = ['b', 'r', 'g', 'y', 'c', 'm', 'y', 'k', 'c', 'g', 'r'], autopct = "%1.2f%%")

plt.axis("equal")

# 显示图例
plt.legend()

# 显示图像
plt.show()
```

![image-20210905202201244](.\pic\image-20210905202201244.png)





------



## Numpy

- 什么是Numpy？

    Numpy是一个开源的python科学计算库，用于快速处理任意维度的数组，Numpy支持常见的数组和矩阵操作。对于同样的数值计算任务，使用Numpy要简洁的多。

    Numpy使用ndarray对象来处理多维数组，该对象是一个快速而灵活的大数据的容器。

- 什么是ndarray？

    Numpy提供了一个N维数组类型的ndarray，它描述了相同类型的 ‘’ items ’‘ 的集合，对数组的存储效率和输入输出性能远优于python中的嵌套列表，数组越大，Numpy的优势就越明显。

- ndarray的优势

    - 内存块风格
        - ndarray中所有的元素的类型都是相同的，而python列表中的元素类型是任意的，所以ndarray在存储元素的时内存可以连续，而python原生的list就只能通过寻址的方式找到下一个元素，这虽然也导致了在通用性能方面Numpy的ndarray不及Python原生的list，但是在科学运算中，Numpy的ndarra就可以省去很多的循环语句，代码使用方面比Python的原生的list简单的多。
    - 并行化运算
        - ndarray支持并行化运算（向量化运算）
    - 底层语言
        - Numpy底层使用c语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制，效率远高于纯python代码。

### ndarray的属性

```python
ndarray.shape	数组维度的元组
ndarray.ndim	数组维数
ndarray.size	数组中元素的数量
ndarray.itemsize	一个数组元素的长度（字节）
ndarray.dtype	数组元素的类型
```

```python
import numpy as np

score = np.array([
    [80, 89, 86, 67, 79],
    [78, 97, 89, 67, 81],
    [90, 94, 78, 67, 84],
    [91, 91, 90, 67, 69],
    [76, 87, 75, 67, 86],
    [70, 79, 84, 67, 84],
    [94, 92, 93, 67, 64],
    [86, 85, 83, 67, 80]
])

score.shape # (8, 5)
score.ndim # 2
score.size # 40
score.dtype # dtype('int32')
score.itemsize # 4
```

### ndarray形状

```python
import numpy as np

a = np.array([[1,2, 3], [4, 5, 6]])
b = np.array([1, 2, 3, 4])
c = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])

a.shape # (2, 3)
b.shape # (4, )
c.shape # (2, 2, 3)
```

### ndarray类型

![image-20210906151555036](.\pic\image-20210906151555036.png)

```python
import numpy as np

a = np.array([1, 2, 3, 4, 5])
b = np.array([1.1, 1.2, 1.3])

a.dtype  # dtype('int32')
b.dtype  # dtype('float64')

c = np.array([1, 2, 3, 4], dtype = np.float64)
c.dtype  # dtype('float64')
```

### 生成数组的方法

#### 生成0和1的数组

```python
np.zeros(shape)
np.ones(shape)
```

```python
import numpy as np

np.zeros(shape = (3, 4), dtype = "float32")
np.ones(shape = [2, 3], dtype = np.int32)
```

#### 从现有数组中生成

```python
np.array() # 深拷贝
np.copy() # 深拷贝
np.asarray() # 浅拷贝
```

#### 生成固定范围的数组

```python
np.linspace(0, 10, 1000) # 生成0~1区间段有1000个数，生成的特点是 左闭右闭，等距离的 
np.arange(a, b, c) # 左闭右开 c是步长 
```

```python
import numpy as np

np.linspace(0, 10, 5)  # array([ 0. ,  2.5,  5. ,  7.5, 10. ])
np.arange(0, 10, 5) # array([0, 5])
```

#### 生成随机数组

##### 均匀分布

```python
np.random.uniform(low=0.0, high=1.0, size=None) # 从一个均匀分布的[low， high)中随机采样，注意定义域是左闭右开
```

```python
import numpy as np
import matplotlib.pyplot as plt

data1 = np.random.uniform(low=-1, high=1, size = 1000000)

plt.figure(figsize=(20, 8), dpi = 80)

plt.hist(data1, 1000)

plt.show()

```

![image-20210906181351166](.\pic\image-20210906181351166.png)

##### 正态分布

正态分布是一种概率分布。正态分布是具有两个参数∪和⊕的连续型随机变量的分布,第一个参数∪是服从正太分布的随机变量的均值,第二个参数⊕是此随机变量的标准差 ,所以正态分布记作N(∪,⊕)

![image-20210906182431339](.\pic\image-20210906182431339.png)

 

```python
np.random.normal(loc=0.0, scale=1.0, size=None)
```

```python
import numpy as np
import matplotlib.pyplot as plt

data2 = np.random.normal(loc=1.75, scale=0.1, size = 1000000)

plt.figure(figsize=(20, 8), dpi = 80)

plt.hist(data2, 1000)

plt.show()

```

![image-20210906183246707](.\pic\image-20210906183246707.png)

### 基本操作

#### 获取数据

```python
 # 随机生成8只股票2周的交易日涨幅数据
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))
store_change
# 获取第一个股票前3个交易日的涨跌幅数据
store_change[0, 0:3]

a1 = np.array([[[1, 2, 3], [4, 5, 6]], [[12, 3, 34], [5, 6, 7]]]) # (2, 2, 3)
a1[1, 0, 2] # 34
```

#### 形状修改

```python
ndarray.reshape(shape) # 只是修改了形状,返回新的ndarray，原始数据没有改
ndarray.resize(shape) # 只改变形状，没有返回值，对原始的ndarray进行修改
# 自动计算 shape(-1, b)
ndarray.T # 转置
```



```
# 需求：让刚才的股票行，日期列反过来，变成日期行，股票列
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))
store_change
# 获取第一个股票前3个交易日的涨跌幅数据
store_change[0, 0:3]

store_change.T
```

#### 类型的修改

```python
ndarray.astype(type)
ndarray.toString() # ndarray序列化到本地
```

#### 数组的去重

```python
ndarray.unique() # 去重
```

### ndarray的运算

#### 逻辑运算

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))

# 如果涨跌幅大于0.5就标记为true,否则为false

store_change > 0.5
/*
array([[False, False,  True, False, False, False,  True, False,  True,
        False],
       [False, False, False,  True, False, False, False,  True, False,
        False],
       [False, False, False, False, False, False, False,  True, False,
        False],
       [ True,  True, False, False,  True, False, False, False, False,
        False],
       [ True,  True,  True, False, False, False, False, False, False,
        False],
       [ True, False,  True, False, False, False,  True, False, False,
         True],
       [False, False, False,  True,  True, False, False, False, False,
        False],
       [False,  True,  True, False, False, False, False,  True, False,
        False]])
*/


```

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))

# 如果涨跌幅大于0.5就标记为true,否则为false

# store_change > 0.5

store_change[store_change > 0.5] = 1.1
store_change
/*
array([[ 7.77778246e-02, -2.84650998e+00,  1.10000000e+00,
        -6.62919583e-01, -4.93193322e-01, -3.92657789e-01,
        -1.42678471e+00, -8.27906236e-01, -6.17788476e-01,
         1.10000000e+00],
       [-1.45580340e+00, -2.34808317e-01,  1.10000000e+00,
        -1.58087059e+00, -6.06375323e-01,  4.12311261e-01,
        -1.07785235e+00,  4.32473676e-02, -1.17852930e+00,
        -1.73899883e+00],
					··············
       [ 1.10000000e+00,  1.10000000e+00,  1.10000000e+00,
        -1.44664443e+00,  3.01976087e-01, -9.48075071e-01,
        -4.33025390e-01,  1.19762722e-01,  2.85128714e-01,
        -1.08100253e+00],
       [-3.05908988e-01, -1.63310263e+00,  1.10000000e+00,
        -5.17473855e-02,  3.97517613e-01,  1.98748821e-02,
         4.05582814e-01, -8.17822018e-01,  4.07352207e-01,
         3.83072097e-01]])
*/
```

##### 通用判断函数

```python
np.all(布尔值) # 只要有一个False就返回false，只有全是True才返回true
np.any(布尔值) # 只要有一个True就返回True，只有全部为False才返回False
```

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))

# 判断store_change[0:2, 0:5]是否全是上涨的
np.all(store_change[0:2, 0:5] > 0) # False

# 判断前五支股票这段期间是否有上涨
np.any(store_change[:5, :] > 0) # True
```

##### 三元运算符

```python
np.where(布尔值, True的位置的值, False的位置的值) 
np.logical_and(条件1， 条件2) # 逻辑运算符
np.logical_or(条件1， 条件2)
```

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))

temp = store_change[:4, :4]
np.where(temp > 0, 1, 0)
/*
array([[1, 1, 1, 1],
       [1, 1, 0, 0],
       [0, 1, 0, 1],
       [0, 1, 0, 0]])
*/
```



#### 统计运算

##### 统计指标函数

min, max, mean(平均值), median, var(方差), std(标准差)

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))


# 判断前四个股票前四天的涨跌幅 大于0的置为1 否则为0
temp = store_change[:4, :4]

# np.max(temp)
temp.max(axis = 0) # 0按列求最大值  1 按行求最
```

##### 返回最大值，最小值所在位置

```python
np.argmax(temp, axis=)
np.argmin(temp, axis=)
```

```python
import numpy as np
import matplotlib.pyplot as plt

store_change = np.random.normal(loc=0, scale=1, size=(8, 10))

temp = store_change[:4, :4]

np.argmax(temp, axis=-1) # array([2, 2, 3, 0], dtype=int64)
```

#### 数组间运算

##### 数组与数的运算

```python
ndarray + 1
运算符等
```

##### 数组与数组的运算

要求两个ndarray 满足广播机制

```
ndarray1 + ndarray2
```

##### 广播机制

**执行broadcast的前提在于，两个ndarray执行的是element-wise的运算，Broadcast机制的功能是为了方便不同形状的ndarray进行数学运算，当操作两个数组是，numpy会逐个比较它们的shape，只有在下述情况下，两个数组才能够进行数组与数组的运算。**

- **维度相等**
- **shape（其中相对应的一个地方为1）**

 ![image-20210906211323832](.\pic\image-20210906211323832.png)

##### 矩阵运算

两种方法存储矩阵：

ndarray 二维数组

matrix 数据结构  np.mat(**)

**矩阵乘法运算：**

形状：（m, n） * (n, l)  = (m, l)

**运算规则**:

- 如果是ndarray的结构：

    ​	np.matnul(array, array)

    ​	np.dot(array, array)

- 如果是matrix的结构:

    ​	mat1 * mat2



#### 合并，分割的用处

```python
numpy.hstack(tup) # 水平
numpy.vstack(tup) # 竖直
numpy.concatenate((a1, a2, a3,...), axis=0) # 0是竖直拼接
numpy.split(ary, indices_or_sections, axis=0)
```

### IO操作和数据处理

```python
texnp.genfromtxt("***", delimiter=",")
```

------



## Pandas

什么是Pandas？

- 2008年WesMckinney开发的库
- 专门用于数据挖掘的开源python库
- 以Numpy为基础，借力Numpy模块在计算方面性能高的优势
- 基于matplotlib，能够简便的画图
- 独特的数据结构

核心数据结构：

- DataFrame
- Panel
- Series

### DataFrame

- 结构：既有行索引，又有列索引的二维数组

```python
# -*- encoding: utf-8 -*-
"""
@File    : demo01_DataFrame.py
@Time    : 2021/9/7 20:28
@Author  : 岳昌宏
@Email   : 2291890518@qq.com
@Software: PyCharm
"""

import numpy as np
import pandas as pd

# 创建一个符合正态分布的10个股票的5天的涨跌幅数据
stock_change = np.random.normal(0, 1, (10, 5))

# 添加行列索引
stock = ["股票{}".format(i) for i in range(10)] # 添加行索引
date = pd.date_range(start="20180101", periods=5, freq="B")# 添加列索引
dataframe = pd.DataFrame(stock_change, stock, date)

print(dataframe)

/*
输出结果：
     2018-01-01  2018-01-02  2018-01-03  2018-01-04  2018-01-05
股票0   -0.802365   -0.633436   -0.685347    0.193362    0.685613
股票1   -1.273951   -0.936942   -0.097411    0.186454    0.109738
股票2   -0.517009    1.612196   -0.390844   -1.290394    0.418181
股票3   -0.092196   -0.254635    1.012643    2.034280    0.656286
股票4    1.067216   -0.462618    0.398016   -1.537230   -0.539264
股票5    0.227965    0.266802   -0.220648   -0.351347    1.050506
股票6   -0.108396    0.702085   -3.640980    1.126790    0.552895
股票7   -0.828635   -0.061979    0.836997    0.452572   -0.777004
股票8    2.225811    1.058938   -2.073572    0.287487   -0.178675
股票9   -0.820033   -0.245704    1.162504    0.391300   -1.110940

*/
```

- 属性：
    - shape：形状
    - index：DataFrame的行索引列表
    - columns：DataFrame的列索引列表
    - values：直接获取其中array的值
    - T：对行列的转置

```python
print(dataframe.shape) # (10, 5)
print(dataframe.values)
print(dataframe.index) # Index(['股票0', '股票1', '股票2', '股票3', '股票4', '股票5', '股票6', '股票7', '股票8', '股票9'], dtype='object')
print(dataframe.columns)
'''
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
               '2018-01-05'],
              dtype='datetime64[ns]', freq='B')
'''

print(dataframe.T)
```

- 方法：
    - head(num)：默认返回前五行
    - tail(num)：默认返回后五行



#### DataFrame索引值的设置

- 修改行列索引值
    - 不能单独修改索引，要想修改必须全部的修改

```python
stock_ = ["股票_{}".format(i) for i in range(10)]

dataframe.index = stock_
/*
输出结果：
      2018-01-01  2018-01-02  2018-01-03  2018-01-04  2018-01-05
股票_0   -0.922084   -0.314697    0.681554   -0.113521    0.030318
股票_1   -1.105473    0.434260    1.779841   -0.362668    1.028760
股票_2    0.464742   -1.544942    0.288548   -0.047757    1.682909
股票_3   -1.658261    1.716322   -0.235597    0.944028   -0.485395
股票_4    1.819818   -0.573014   -1.618524   -1.296449   -1.341305
股票_5    0.405877    0.542087    0.666533   -0.340536    0.230973
股票_6    0.099198   -0.487725   -0.176512   -1.545197    0.102772
股票_7   -1.209917    1.320856   -0.194675    1.811766   -1.002365
股票_8   -0.740776   -0.181764    1.310004   -2.025589   -0.649663
股票_9   -1.407518   -0.472652    1.156162    1.383231    0.382752
*/
```

- 重设索引

```python
dataframe.reset_index(drop=False)
```

- 设置新索引

```python
dataframe2 = pd.DataFrame({'month':[1, 4, 7, 10], 'year':[2021, 2022, 2023, 2024], 'sale':[55, 40, 84, 31]})
print(dataframe2)

/*
   month  year  sale
0      1  2021    55
1      4  2022    40
2      7  2023    84
3     10  2024    31
*/

dataframe2 = dataframe2.set_index("month", drop=False)
print(dataframe2)

/*
       month  year  sale
month                   
1          1  2021    55
4          4  2022    40
7          7  2023    84
10        10  2024    31
*/

dataframe2 = dataframe2.set_index("month", drop=True)
print(dataframe2)

/*
       year  sale
month            
1      2021    55
4      2022    40
7      2023    84
10     2024    31
*/

dataframe2 = dataframe2.set_index(["year", "sale"], drop=True)
print(dataframe2)

/*
           month
year sale       
2021 55        1
2022 40        4
2023 84        7
2024 31       10
*/
```

### Multilndex与Panel

#### MultiIndex

- index的属性
    - names：levels的名称
    - levels：每个level的元组值

```python
dataframe2 = dataframe2.set_index(["year", "sale"], drop=True)
/*
           month
year sale       
2021 55        1
2022 40        4
2023 84        7
2024 31       10
*/

print(dataframe2.index)
/*
MultiIndex([(2021, 55),
            (2022, 40),
            (2023, 84),
            (2024, 31)],
           names=['year', 'sale'])
*/
print(dataframe2.index.names) # ['year', 'sale']
print(dataframe2.index.levels) # [[2021, 2022, 2023, 2024], [31, 40, 55, 84]]
```

#### Panel

```python
p = pd.Panel(data=np.arange(24).reshape(4,3,2),
                 items=list('ABCD'),
                 major_axis=pd.date_range('20130101', periods=3),
                 minor_axis=['first', 'second'])
 
# 结果
<class 'pandas.core.panel.Panel'>
Dimensions: 4 (items) x 3 (major_axis) x 2 (minor_axis)
Items axis: A to D
Major_axis axis: 2018-01-01 00:00:00 to 2018-01-03 00:00:00
Minor_axis axis: first to second

#  python3是读取不了上面的数据的
```

### Series

带索引的一维数组,series结构只有行索引

```python
print(dataframe.iloc[1, :])

/*
2018-01-01    0.858577
2018-01-02   -0.921147
2018-01-03    0.028350
2018-01-04   -0.362713
2018-01-05    0.354388
Freq: B, Name: 股票1, dtype: float64
*/
```

- 属性
    - index
    - values

```python
ser = pd.Series(np.array(range(3, 9, 2)))
print(ser)

/*
0    3
1    5
2    7
dtype: int32
*/

ser2 = pd.Series(np.array(range(3, 9, 2)), index=["a", "b", "c"])
print(ser2)

/*
a    3
b    5
c    7
dtype: int32
*/

ser3 = pd.Series({"A":11, "B":22, "C":33})
print(ser3)

/*
A    11
B    22
C    33
dtype: int64
*/
```

可以理解说DataFrame是Series的容器，Panel是DataFrame的容器

### 基本数据操作

```python
import numpy as np
import pandas as pd

data = pd.read_csv("../file/stock_day.csv")
# 删除一些列，让数据更简单些
data = data.drop(["ma5", "ma10", "ma20", "v_ma5", "v_ma10", "v_ma20"], axis=1) # 按列
```

- 索引操作

```python
# 直接索引(先列后行)
print(data["open"]["2018-02-26"]) # 22.8
# 按名字索引(先行后列)
print(data.loc["2018-02-26"]["open"]) # 22.8
print(data.loc["2018-02-26", "open"]) # 22.8
# 按数字索引
print(data.iloc[1, 0]) # 22.8
# 组合索引
# print(data.ix[:4, 'open', 'close'])  # 已经删除
```

- 赋值操作

```python
data.open = 100 # 对象.属性
data.iloc[1, 0] = 22

print(data)
```

- 排序操作
    - 对内容排序
        - dataframe
        - series
    - 对索引排序
        - dataframe
        - series

```python
data.sort_values(by="high", ascending=True) # 默认升序，按照一个字段进行排序

data.sort_values(by=["high", "p_change"], ascending=True) # 按照两个字段进行排序

data.sort_index()
```

### 算术运算与逻辑运算

- 算术运算
    - data["open"].add(3)  / data["open"] + 3
    - data["open"].sub(3) / data["open"] - 3
    - .....
    - 也可以一一对应的算术
- 逻辑运算
    - 逻辑运算符
        - <,> 返回布尔值
        - data["p_change"] > 2
        - data[data["p_change"] > 2] 
    - 逻辑运算函数
        - query()
            - data.query("p_change" > 2 & "low" > 15)
        - isin() : 判断是否包含返回布尔值
            - data["turnover"].isin([4.19, 2.39])
- 统计运算
    - min max mean median var std
    - describe()
    - idxmax() : 获取最大知道额位置
    - idxmin()
    - 累加统计函数
        -  cumsun
- 自定义运算
    - apply(func,axis = 0) : func自定义函数。默认是按照列进行运算
        - data.apply(lambda x : x.max() - x.min())



### Pandda画图

.plot()

```python

data.plot("p_change", "turnover", "scatter")

plt.show()
```

![image-20210907224902638](.\pic\image-20210907224902638.png)

### 文件的读取与存储

![image-20210908111158160](.\pic\image-20210908111158160.png)

#### CSV

```python
import numpy as np
import pandas as pd

data = pd.read_csv("../file/stock_day.csv", usecols=['open', 'high', 'colse'])
# 删除一些列，让数据更简单些
# data = data.drop(["ma5", "ma10", "ma20", "v_ma5", "v_ma10", "v_ma20"], axis=1) # 按列
```

![image-20210908111947186](.\pic\image-20210908111947186.png)

```python
# 如果表中没有索引，会将第一行作为字段

data = pd.read_csv("../file/stock_day2.csv", names=[字段])
```

- 存储csv

```python
# 保存‘open’列的数据

data[:10].to_csv(path, column=['要保存的列'], index=False, mode="a", head=False) # 不要行索引,追加模式,不要追加头所有
```

#### HDF5

hdf5存储3维数据的文件

key1 dataframe1

key2 dataframe2

....

```python
pd.read_hdf('path', key='**') # 当有一个key时，可以不指定key

df.to_hdf('path1',key='**1')
df.to_hdf('path1',key='**2')
```

HDF5在存储的是支持压缩，使用的方式是blosc，这个是速度最快的也是pandas默认支持的

使用压缩可以提高磁盘利用率，节省空间

HDF5还是跨平台的，可以轻松迁移到hadoop上面

#### JSON

Json是一种数据交换格式，前后端交互经常用到，也会在存储的时候选择这种格式，所以我们需要知道Pandas如何进行读取和存储Json格式。

```python
pd.read_json('path', orient='records', lines=True) # 读取格式，是否按行读取

data.to_json('path',orient='records', lines=True)
```

## Pandas高级处理

### 缺失值处理

- 如何进行缺失值nan处理：
    - 删除含有缺失值的样本
    -  替换/插补
- 如何处理nan：
    - 判断数据中是否存在nan：``pd.isnull(df) / pd.notnull(df)``
    - 删除含有缺失值的样本``df.dropna(axis='rows', inplace=True/False)`` 默认是按行删除。如果是True就就地删除，如果是False不会修改df，而是返回一个删除后的df，默认是False
    - 替换插补``df.fillna(value, inplace=True/False)``
- 如果缺失值不是nan而是？号等其他标记：
    - 替换``df.replace(to_replace=, value=np.nan)``to_replace:替换前的值，value替换后的值
    - 把其他符合替换成nan

```python
'''
是否存在nan类型的缺失值
'''
import pandas as pd
import numpy as np

# 读取数据
movie = pd.read_csv("../file/IMDB-Movie-Data.csv")

# 判断是否存在缺失值
nan = pd.isnull(movie).any()

# 缺失值处理
# 删除含有缺失值的样本
# movie.dropna(inplace=True)
# 替换
movie["Revenue (Millions)"].fillna(movie["Revenue (Millions)"].mean(), inplace=True)
movie["Metascore"].fillna(movie["Metascore"].mean(), inplace=True)

nan2 = pd.isnull(movie).any()

print(nan)
print(nan2)
print(movie)
'''
Rank                  False
Title                 False
Genre                 False
Description           False
Director              False
Actors                False
Year                  False
Runtime (Minutes)     False
Rating                False
Votes                 False
Revenue (Millions)     True
Metascore              True
dtype: bool
Rank                  False
Title                 False
Genre                 False
Description           False
Director              False
Actors                False
Year                  False
Runtime (Minutes)     False
Rating                False
Votes                 False
Revenue (Millions)    False
Metascore             False
dtype: bool
     Rank                    Title  ... Revenue (Millions) Metascore
0       1  Guardians of the Galaxy  ...         333.130000      76.0
1       2               Prometheus  ...         126.460000      65.0
2       3                    Split  ...         138.120000      62.0
3       4                     Sing  ...         270.320000      59.0
4       5            Suicide Squad  ...         325.020000      40.0
..    ...                      ...  ...                ...       ...
995   996     Secret in Their Eyes  ...          82.956376      45.0
996   997          Hostel: Part II  ...          17.540000      46.0
997   998   Step Up 2: The Streets  ...          58.010000      50.0
998   999             Search Party  ...          82.956376      22.0
999  1000               Nine Lives  ...          19.640000      11.0
'''
```

```python
'''
是否存在不是nan类型的缺失值
'''
import pandas as pd
import numpy as np

# 读取数据
data = pd.read_csv("../file/IMDB-Movie-Data.csv")

# 替换 ？ -> np.nan
data_new = data.replace(to_replace="?", value=np.nan)

# 按照nan的方式进行处理
# 删除缺失值
data_new.dropna(replace=True)

print(data_new)
```



### 数据离散化

- 什么是数据离散化？

one-hot编码 / 哑变量

![image-20210909192823604](.\pic\image-20210909192823604.png)

- 为什么要数据的离散化？
    - 连续属性离散化的目的是为了简化数据结构，数据离散化技术可以用来减少给定连续属性值的个数，离散化方法经常作为数据挖掘的工具
- 如何实现数据的离散化？
    - 分组
        - 自动分组 ``sr = pd.qcut(data, bins)``
        - 自定义分组``sr = pd.cut(data, [])``
    - 将分组好的结果转换成哑变量
        - ``pd.get_dummies(sr, prefix='前缀', )``

```python
import numpy as np
import pandas as pd

# 准备数据
data = pd.Series([165, 174, 160, 180, 159, 163, 192, 184], index=['No1:165','No2:174','No3:160','No4:180','No5:159','No6:163','No7:192','No8:184'])
# 分组
# 自动分组
# sr = pd.qcut(data, 3)
# 自定义分组
sr = pd.cut(data, [150, 165, 180, 195])
print(sr.value_counts())
print("------")
# 转换成哑变量
data2 = pd.get_dummies(sr, prefix="身高")

print(data2)

'''
(150, 165]    4
(180, 195]    2
(165, 180]    2
dtype: int64
------
         身高_(150, 165]  身高_(165, 180]  身高_(180, 195]
No1:165              1              0              0
No2:174              0              1              0
No3:160              1              0              0
No4:180              0              1              0
No5:159              1              0              0
No6:163              1              0              0
No7:192              0              0              1
No8:184              0              0              1

Process finished with exit code 0
'''
```



### 合并

- 按方向拼接``pd.concat([data1, data2], axis=0)``默认竖直拼接（行索引一样水平拼接， 列索引一样竖直拼接）
- 按索引拼接``pd.merge(left, right, how="inner", on=[索引])``
    - ![image-20210909203910918](.\pic\image-20210909203910918.png)
    - ![image-20210909203857025](.\pic\image-20210909203857025.png)
    - ![image-20210909204134598](.\pic\image-20210909204134598.png)

### 交叉表和透视表

- 交叉表：交叉表用于计算一列数据对于另一列数据的分组个数（寻找两个列之间的关系）
    - ``pd.crosstab(value1, value2)``

 

```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 星期数据以及涨跌幅是好是坏数据
stock = pd.read_csv("../file/stock_day.csv")
# 准备星期数据列
date = pd.to_datetime(stock.index)
stock["week"] = date.weekday
# 准备涨跌幅数据列
stock["pona"] = np.where(stock["p_change"] > 0, 1, 0)

# 调用交叉表
cross = pd.crosstab(stock["week"], stock["pona"])

cross_sum = cross.div(cross.sum(axis=1), axis = 0)

cross_sum.plot(kind="bar", stacked=True)


plt.show()
print(cross_sum)

'''
pona         0         1
week                    
0     0.504000  0.496000
1     0.419847  0.580153
2     0.462121  0.537879
3     0.492188  0.507812
4     0.464567  0.535433
'''
```

![image-20210909211150545](.\pic\image-20210909211150545.png)

- 透视表``data.prvot_table([], index=[])``

### 分组与聚合

- 分组

```python
import pandas as pd
import numpy as np

col = pd.DataFrame({'color': ['white','red','green','red','green'], 'object': ['pen','pencil','pencil','ashtray','pen'],'price1':[5.56,4.20,1.30,0.56,2.75],'price2':[4.75,4.12,1.60,0.75,3.15]})

# 进行分组，对颜色分组，price进行聚合
# 用dataframe进行分组
# col = col.groupby(by="color")["price1"].max()
col = col["price1"].groupby(col["color"]).max()

print(col)
'''
color
green    2.75
red      4.20
white    5.56
Name: price1, dtype: float64
'''
```





------



## 决策树

决策树是一种**非参数**（不限制数据的结构和类型）的有**监督的学习**方法，它能够从一系列有**特征**和**标签**的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。

在决策过程中，我们一直对记录的特征进行提问，最初的问题所在的地方叫做**根节点**，在得到结论前的每一个问题都是**中间节点**，而得到的每一个结论都叫做**叶子节点**。



根节点：没有进边，有出边。包括最初，针对特征的提问。

中间节点：既有进边也有出边，进边只有一条，出边可以有很多条，都是针对特征提问的。

叶子节点：有进边，没有出边，**每个叶子节点都是一个类别的标签**



- 决策树算法的核心是要解决两个问题？
    - 如何从数据表中找出最佳节点和最佳分支
    - 如何让决策树停止生长，防止过拟合？



- sklearn.tree

![image-20210911160407198](.\pic\image-20210911160407198.png)



- sklearn建模的流程
    1. 实例化，建立模型评估对象		——实例化时需要使用的参数
    2. 通过模型的接口训练模型     ——数据属性，数据接口
    3. 通过模型接口提取需要的信息    ——数据属性，数据接口



### DecisionTreeClassifier与红酒数据集

- **criterion**
    - 不纯度基于节点来计算的，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一颗决策树上，叶子节点的不纯度一定是最低的。
    - entropy ： 使用信息熵
    - gini ：使用基尼系数
- **random_state**:用来设置分枝中随机模式的参数，默认是None。在高维度时，随机性会表现的更明显，低维度的数据，随机性几乎不明显。输入任意的整数，会长出同一颗树，让模型稳定下来。
- **splitter**:也是用来控制决策树的随机选项的，有两种输入值，输入**“best”**，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（更重要的分枝可以通过属性feature_importance_查看），输入**“random”**,决策树在分枝时会更加随机，树会更深，对训练集的拟合将会降低，这也是防止过拟合的一种方式。

为了让决策树有更好的范化性，我们要对决策树进行剪枝，**剪枝策略**对决策树的影响巨大，正确的剪枝策略是优化决策树算法的核心

- **max_depth:** 限制树的深度，超过设定深度的树枝全部剪掉，这是用得最广泛的剪枝参数，这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。

- **min_samples_leaf&min_samples_splits**:

    - min_samples_leaf一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生

    - min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则

        分枝就不会发生。

- **max_features&min_impurity_decrease:**

    - max_features:限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，

        max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较**暴力**，是直接限制可以使用的特征数量

        而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型

        学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。

    - **min_impurity_decrease:**限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的

        功能，在0.19版本之前时使用min_impurity_split

- **class_weight&min_weight_fraction_leaf:**完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给

    少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给

    与数据集中的所有标签相同的权重。

    有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_

    weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_

    fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使

    用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

- **重要属性和接口：**
    - **clf.apply(Xtest**)： 返回每个测试样本所在的叶子节点的索引
    - **clf.predict(Xtest)**：返回每个测试样本的分类/回归结果
    - 所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。sklearn不接受任何一维矩阵作为特征矩阵被输入

```python
# -*- encoding: utf-8 -*-
"""
@File    : DecisionTreeClassifier.py
@Time    : 2021/9/12 11:18
@Author  : 岳昌宏
@Email   : 2291890518@qq.com
@Software: PyCharm
"""

import graphviz
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import pandas as pd

# 红酒数据集
wine = load_wine()

print("数据集的结构：" , wine.data.shape)
print("数据集的特征：" , wine.target)
print("-" * 100)
data = pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=1)
print(data)
print("-" * 100)
# 标签的名字
print(wine.target_names)
# 特征的名字
print(wine.feature_names)

print("*" * 100)

# 划分测试集和训练集
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)

clf = tree.DecisionTreeClassifier(criterion="entropy",
                                  random_state = 30,
                                  splitter = "random",
                                  max_depth=3,
                                # min_samples_leaf=10,
                                  min_samples_split=10)
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) # 返回预测的准确度
print(score)

# 可视化
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']

dot_data = tree.export_graphviz(
    clf,
    feature_names = feature_name,
    class_names = ["琴酒","雪莉","贝尔莫得"],
    filled = True, # 是否填充颜色，颜色越深不纯度越低
    rounded = True # 框的形状
)

graph = graphviz.Source(dot_data)

'''
数据集的结构： (178, 13)
数据集的特征： [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
----------------------------------------------------------------------------------------------------
        0     1     2     3      4     5   ...    8      9     10    11      12  0 
0    14.23  1.71  2.43  15.6  127.0  2.80  ...  2.29   5.64  1.04  3.92  1065.0   0
1    13.20  1.78  2.14  11.2  100.0  2.65  ...  1.28   4.38  1.05  3.40  1050.0   0
2    13.16  2.36  2.67  18.6  101.0  2.80  ...  2.81   5.68  1.03  3.17  1185.0   0
3    14.37  1.95  2.50  16.8  113.0  3.85  ...  2.18   7.80  0.86  3.45  1480.0   0
4    13.24  2.59  2.87  21.0  118.0  2.80  ...  1.82   4.32  1.04  2.93   735.0   0
..     ...   ...   ...   ...    ...   ...  ...   ...    ...   ...   ...     ...  ..
173  13.71  5.65  2.45  20.5   95.0  1.68  ...  1.06   7.70  0.64  1.74   740.0   2
174  13.40  3.91  2.48  23.0  102.0  1.80  ...  1.41   7.30  0.70  1.56   750.0   2
175  13.27  4.28  2.26  20.0  120.0  1.59  ...  1.35  10.20  0.59  1.56   835.0   2
176  13.17  2.59  2.37  20.0  120.0  1.65  ...  1.46   9.30  0.60  1.62   840.0   2
177  14.13  4.10  2.74  24.5   96.0  2.05  ...  1.35   9.20  0.61  1.60   560.0   2

[178 rows x 14 columns]
----------------------------------------------------------------------------------------------------
['class_0' 'class_1' 'class_2']
['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']
****************************************************************************************************
0.9444444444444444

Process finished with exit code 0
'''
```

![image-20210912124433539](.\pic\image-20210912124433539.png)

### DecisionTreeRegressor

几乎所有的参数，属性以及接口都和分类树一模一样，需要注意的是，在回归树中， 没有标签是否均衡的问题，因此没有class_weight这样的参数。

-  **criterion**：

    - 输入"**mse**"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为

        特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失

    - 输入“**friedman_mse**”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差

    - 输入"**mae**"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失

        属性中最重要的依然是feature_importances_，接口依然是apply, fifit, predict, score最核心

- **交叉验证**：交叉验证是用来观察模型的稳定的一种方法，我们将数据划分为n份，依次使用其中的一份作为测试集，其他的n-1份作为训练集，多次计算模型的精确度来评估模型的平均准确程度，训练集和测试集的划分会干扰模型的结果，因此用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量

![image-20210912153418302](.\pic\image-20210912153418302.png)

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor

boston = load_boston() # target是连续的
regressor = DecisionTreeRegressor(random_state=0) # 实例化
cross_val_score(regressor, # 实例化好的模型
                boston.data,
                boston.target,
                cv=5,
                scoring = "neg_mean_squared_error"  # 均方误差（越接近0越好）， 如果注释掉默认返回R平方
               )
'''
array([-12.15176471, -52.12217822, -27.85663366, -52.6449505 ,
       -63.19168317])
'''
```



### 用回归树拟合正弦曲线

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt


#np.random.rand(数组结构)，生成随机数组的函数
rng = np.random.RandomState(1) # 随机数种子 （0， 1）
X = np.sort(5 * rng.rand(80,1), axis=0)  # 生成80行1列的，并且排序
y = np.sin(X).ravel()  # np.sin(X) 生成是（80， 1）二维的，使用ravel给np.sin(X)降维

plt.figure()
plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="data")
```

![image-20210912171609765](.\pic\image-20210912171609765.png)

```python
# 在y上加上噪声
y[::5] += 3 * (0.5 - rng.rand(16))

plt.figure()
plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="data")
```

![image-20210912171642747](.\pic\image-20210912171642747.png)

```python
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
clf1 = regr_1.fit(X, y)
clf2 = regr_2.fit(X, y)

# 准备测试集
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
# np.arange(a, b, c) # 左闭右开 c是步长  [:,np.newaxis] 增维(多行一列) [np.newaxis, :] 也是增维（一行多列）
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)

# 画图
plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)

plt.xlabel("data")
plt.ylabel("target")

plt.title("Decision Tree Regression")
plt.legend()
plt.show()
```

![image-20210912171801744](.\pic\image-20210912171801744.png)

> [:,np.newaxis] 增维(多行一列) [np.newaxis, :] 也是增维（一行多列）

### 泰坦尼克号生存者预测

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
```

导入数据

```python
data = pd.read_csv(r"./data/data.csv")
data.info() # 查看数据的详细内容，发现有缺失值
```

![image-20210912205237516](.\pic\image-20210912205237516.png)

```python
data.head() # 默认显示前5行
```

![image-20210912205413783](.\pic\image-20210912205413783.png)

```python
# 筛选特征
data.drop(['Cabin', 'Name', 'Ticket'], inplace=True,axis=1) 
# 第二个参数为true，表示：请用删除之后的表来覆盖原表, axis=1表示对列进行操作,如果多次运行会报错
```

```python
# 处理缺失值
data["Age"] = data["Age"].fillna(data["Age"].mean()) # 使用均值对缺失值进行填补
data = data.dropna(axis=0) # 删除有缺失的行
data.info()
```

![image-20210912205629107](.\pic\image-20210912205629107.png)

```python
# 把汉字换为数字
labels = data["Embarked"].unique().tolist() # [S, C, Q]
data["Embarked"] = data["Embarked"].apply(lambda x:labels.index(x)) # lambda x是这一列每一个数据
# 对性别这一列进行操作
data["Sex"] = (data["Sex"] == "male").astype("int")
# (data["Sex"] == "male")返回True or False 把True or False 这种类型换一种方式展示.astype("int")
```

```python
data.head()
```

![image-20210912205844653](.\pic\image-20210912205844653.png)

```python
# 数据预处理完成，没有字符型，没有缺失值
data.info()
```

![image-20210912205952485](.\pic\image-20210912205952485.png)

```python
# 分离特征和标签
x = data.iloc[:, data.columns != "Survived"] # 取出特征
y = data.iloc[:,data.columns == "Survived"] # 取出标签

from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.3)
```

此时Xtrain, Xtest, Ytrain, Ytest它们的索引是乱序的，所以需要**纠正索引**

```python
# 纠正索引
for i in [Xtrain, Xtest, Ytrain, Ytest]:
    i.index = range(i.shape[0])
    
'''
Xtrain是一个Dataframe类型的属性，其中index表示行索引列表
Xtrain.shape返回的是一个元组，表示该Xtrain有多少行和多少列，元组的第一个元素表示有多个行，第二个元素表示有多少列
'''
```

```python
Xtrain
```

![image-20210912210419595](.\pic\image-20210912210419595.png)

```python
clf = DecisionTreeClassifier(random_state=25) # 设计模型
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)

score
```

```
0.7490636704119851
```

```python
from sklearn.model_selection import cross_val_score

clf = DecisionTreeClassifier(random_state=25)

score = cross_val_score(clf, x, y, cv=10).mean() # 使用交叉验证
score
```

```
0.7469611848825333
```

```python
tr = []
te = []

for i in range(10):
    clf = DecisionTreeClassifier(random_state=25,
                                max_depth = i +1, # 改变树的深度，观察对数据评分的影响
                                 criterion = "entropy"
                                )
    clf = clf.fit(Xtrain, Ytrain)
    score_tr = clf.score(Xtrain, Ytrain) 
    score_te = cross_val_score(clf, x, y, cv = 10).mean() # 交叉验证
    tr.append(score_tr)
    te.append(score_te)

print(max(te))

# 如果训练集的曲线高于测试集的曲线，说明是过拟合的，我们可以剪枝
# 如果训练集的曲线低于测试集的曲线，说明是欠拟合的，
plt.figure()
plt.plot(range(1, 11), tr, color="red", label = "train")
plt.plot(range(1, 11), te, color="blue", label = "test")

plt.xticks(range(1, 11))
plt.legend()
plt.show()
```

![image-20210912210745630](.\pic\image-20210912210745630.png)

```python
# 网格搜索：能够帮助我们调整多个参数的技术，枚举的计数
# 缺点：计算量非常的大。非常的耗费时间
```

```python
import numpy as np

# 本质是一串参数和这些参数对应的，我们希望网格希望来搜索的参数的取值范围
parameters = {"criterion":("gini", "entropy"),
              "splitter":("best", "random"),
              "max_depth":[*range(1, 10)],
              "min_samples_leaf":[*range(1, 50, 5)],
              "min_impurity_decrease":[*np.linspace(0, 0.5, 20)]
             }

clf = DecisionTreeClassifier(random_state=25) # 创建模型
GS = GridSearchCV(clf, parameters, cv=10) # 同时满足了fit，score，cross_val_score
GS = GS.fit(Xtrain, Ytrain)
```

```python
GS.best_params_ # 从我们输入的参数和参数的取值列表中，返回最佳组合
```

```python
{'criterion': 'entropy',
 'max_depth': 6,
 'min_impurity_decrease': 0.0,
 'min_samples_leaf': 6,
 'splitter': 'best'}
```

```python
GS.best_score_ # 网格搜索后的模型的评判标准
```

```python
0.823195084485407
```

## 随机森林

**集成学习**（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通

过在数据上构建多个模型，集成所有模型的建模结果。基本上所有的机器学习领域都可以看到集成学习的身影，在

现实中集成学习也有相当大的作用，它可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预

测疾病的风险和病患者的易感性。在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成

算法的身影也随处可见，可见其效果之好，应用之广

**随机深林和决策树的比较**

```python
clf = DecisionTreeClassifier(random_state = 0) # 决策树
rfc = RandomForestClassifier(random_state = 0) # 随机森林

clf = clf.fit(Xtrain, Ytrain)
rfc = rfc.fit(Xtrain, Ytrain)

score_c = clf.score(Xtest, Ytest)
score_r = rfc.score(Xtest, Ytest)

print("Single Tree:{}".format(score_c), "Random Forset:{}".format(score_r))
```

```python
Single Tree:0.9074074074074074 Random Forset:0.9814814814814815
```



```python
# 交叉验证
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

rfc = RandomForestClassifier(n_estimators=25)
rfc_s = cross_val_score(rfc, wine.data, wine.target, cv = 10) # 交叉验证

clf = DecisionTreeClassifier()
clf_s = cross_val_score(clf, wine.data, wine.target, cv = 10)

plt.plot(range(1, 11), rfc_s, label = "RandomForest")
plt.plot(range(1, 11), clf_s, label = "DecisionTree")

plt.legend()
plt.show()
```

![image-20210913165720699](.\pic\image-20210913165720699.png)



```python
rfc_l = []
clf_l = []

for i in range(10):
    # 随机森林
    rfc = RandomForestClassifier(n_estimators = 25)
    rfc_s = cross_val_score(rfc, wine.data, wine.target, cv = 10).mean()
    rfc_l.append(rfc_s)
    
    # 决策树
    clf = DecisionTreeClassifier()
    clf_s = cross_val_score(rfc, wine.data, wine.target, cv = 10).mean()
    clf_l.append(clf_s)

plt.plot(range(1, 11), rfc_l, label = "Random Forset")
plt.plot(range(1, 11), clf_l, label = "Decision Tree")

plt.legend()
plt.show()
```

![image-20210913165803214](.\pic\image-20210913165803214.png)





### RandomForestClassifier

- criterion：不纯度的衡量指标，有基尼系数和信息熵两种选择
- max_depth：树的最大深度，超过最大深度的树枝都会被剪掉
- min_samples_leaf：一个节点在分枝后的每个叶子节点必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生
- min_samples_split：一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生
- max_features：max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，默认值为总特征个数的开平方取整
- min_impurity_decrease：限制信息熵的大小，信息熵增益小于设定的数值的分枝不会发生
- **n_setimators**：这是森林中树木的数量，即基基评估器的数量，这个参数对随机森林模型的影响是单调的，n_setimators越大，模型的效果往往会越好，但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。

```python
# 学习曲线
superpa = []
for i in range(200):
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1)
    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean()
    superpa.append(rfc_s)
    
print(max(superpa),superpa.index(max(superpa)))
plt.figure(figsize=[20,5])
plt.plot(range(1,201),superpa)
plt.show()
```

![image-20210913165846711](.\pic\image-20210913165846711.png)

- **estimators_**：随机森林的重要属性之一，查看森林中数的状况，**我们观察到当random_state固定的时候，随机森林中生成的是一组固定的树，但每颗树依然是不一样的**，这是用“随机挑选特征进行分枝”的方法得到的随机性。并且我们可以证明，当这种随机性越大的时候，装袋法的效果一般会越好。**用装袋法集成的时候，基分类器应当是相互独立的，是不相同的**。

```python
rfc.estimators_
```

![image-20210913172109764](.\pic\image-20210913172109764.png)

```python
rfc = RandomForestClassifier(n_estimators=20,random_state=2)
rfc = rfc.fit(Xtrain, Ytrain) 

#随机森林的重要属性之一：estimators，查看森林中树的状况
rfc.estimators_[0].random_state
for i in range(len(rfc.estimators_)):
    print(rfc.estimators_[i].random_state)
```

![image-20210913172202988](.\pic\image-20210913172202988.png)

- **bootstrap** **&** **oob_score**：要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。**bootstrap参数默认True，代表采用这种有放回的随机抽样技术**。

![image-20210913183612148](.\pic\image-20210913183612148.png)

​		然而有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能

​		被忽略，一般来说，自助集大约平均会包含63%的原始数据

​		因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为**袋外数据(**out of bag data，简写为oob)。除了我们最开始		就划分好的测试集之外，这些数据也可以被用来作为集成算法的测试集。**也就是说，在使用随机森林时，我们可以不划分测试集和		训练集，只需要用袋外数据来测试我们的模型即可**

​		当然，这也不是绝对的，当n和n_estimators都不够大的时候，很可能就没有数据掉落在袋外，自然也就无法使用oob数据来测试模		型了。

​		如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个		重要属性：oob_score_来查看我们的在袋外数据上测试的结果： 

```python
# 无需划分训练集和测试集
rfc = RandomForestClassifier(n_estimators=25,oob_score=True)

rfc = rfc.fit(wine.data,wine.target) #重要属性oob_score_
rfc.oob_score_ # 袋外数据在这个训练集上的预测数据是多少
```

```
0.9662921348314607
```

- 重要的属性和接口：

```python
rfc.score(Xtest, Ytest) # 返回准确率
rfc.feature_importances_ # 所有特征当中特征的重要程度
rfc.apply(Xtest) # 返回测试集中每个数据在树中叶子节点的索引
rfc.predict(Xtest) # 输入测试集返回预测后的标签
rfc.predict_proba(Xtest) # 每个样本在不同标签中的概率
```

### RandomForestRegressor

回归树衡量分枝质量的指标，支持的标准有三种：

1）输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为

特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失

2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差

3）输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失其中N是样本数量，i是每一个数据样本，fifi是模型回归出的数值，yi是样本点i实际的数值标签。

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import sklearn

boston = load_boston()
regressor = RandomForestRegressor(n_estimators=100,random_state=0)
cross_val_score(regressor, boston.data, boston.target, cv=10,scoring = "neg_mean_squared_error")
# sklearn当中的模型评估指标（打分）列表
sorted(sklearn.metrics.SCORERS.keys())
```

```
['accuracy',
 'adjusted_mutual_info_score',
 'adjusted_rand_score',
 'average_precision',
 'balanced_accuracy',
 'completeness_score',
 'explained_variance',
 'f1',
 'f1_macro',
 'f1_micro',
 'f1_samples',
 'f1_weighted',
 'fowlkes_mallows_score',
 'homogeneity_score',
 'jaccard',
 'jaccard_macro',
 'jaccard_micro',
 'jaccard_samples',
 'jaccard_weighted',
 'max_error',
 'mutual_info_score',
 'neg_brier_score',
 'neg_log_loss',
 'neg_mean_absolute_error',
 'neg_mean_gamma_deviance',
 'neg_mean_poisson_deviance',
 'neg_mean_squared_error',
 'neg_mean_squared_log_error',
 'neg_median_absolute_error',
 'neg_root_mean_squared_error',
 'normalized_mutual_info_score',
 'precision',
 'precision_macro',
 'precision_micro',
 'precision_samples',
 'precision_weighted',
 'r2',
 'recall',
 'recall_macro',
 'recall_micro',
 'recall_samples',
 'recall_weighted',
 'roc_auc',
 'roc_auc_ovo',
 'roc_auc_ovo_weighted',
 'roc_auc_ovr',
 'roc_auc_ovr_weighted',
 'v_measure_score']
```

### 用随机森林回归填补缺失值

在sklearn中，我们可以使用**sklearn.impute.SimpleImputer**来轻松地将均值，中值，或者其他最常用的数值填补到数据中

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_boston # 应用波士顿数据集
from sklearn.impute import SimpleImputer # 用来填补缺失值的类
from sklearn.ensemble import RandomForestRegressor
```

查看波士顿数据集

```python
dataset = load_boston()
dataset.data.shape  # (506, 13)
```

```python
X_full, y_full = dataset.data, dataset.target
n_samples = X_full.shape[0] # 样本的数量
n_features = X_full.shape[1] # 标签的数量
```

```python
# 首先确定我们希望放入的缺失数据的比例，在这里我们假设是50%，那总共就要有3289个数据缺失

rng = np.random.RandomState(0) # 随机数种子
missing_rate = 0.5
n_missing_samples = int(np.floor(n_samples * n_features * missing_rate)) # 缺失数据的个数
# 这里为什么要用floor，因为：如果改变missing_rate的话，就有可能缺失数据的个数是一个小数
# np.floor 向下取整，返回.0格式的浮点数
```

```python
# 所有数据要随机遍布在数据集的各行各列当中，而一个缺失的数据需要一个行索引和一个列索引
# 如果能够创造一个数组，包含3289个分布在0-506中间的行索引，和3289个分布在0-13之间的列索引
# 那我们就可以利用索引来为数据中的任意3289个位置赋空置
# 然后我们用0，均值和随机森林来填写这些缺失值，然后查看回归的结果如何

missing_features = rng.randint(0, n_features, n_missing_samples) # 列索引
# randint(下限， 上限， n) 在上限和下限之间取出n个整数的数组
missing_samples = rng.randint(0, n_samples, n_missing_samples) # 行索引


# missing_samples = rng.choice(n_samples, n_missing_samples, replace = False) （上限，个数）
# 我们现在采样了3289个数据，远远超过了我们的样本的数量506，所以我们使用随机抽取函数randint，但是如果我们需要的数据量小于我们的样本数量506，那
# 我们可以采用np.random.choice来抽样，choice会随机抽取不重复的随机数，因此我们可以帮助我们让数据更加分散，确保数据不会集中在一些行中

```

```python
missing_features
```

```python
array([12,  5,  0, ..., 11,  0,  2])
```

```python
X_missing = X_full.copy()
y_missing = y_full.copy()
```

```python
X_missing[missing_samples, missing_features] = np.nan
X_missing = pd.DataFrame(X_missing) # 转化成DataFrame是为了后续方便的各种操作
```

```python
X_missing
```

![image-20210914212428850](.\pic\image-20210914212428850.png)

```python
# 使用均值进行填补
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values = np.nan, strategy='mean') # 第一步实例化(缺失值的格式， 填补缺失值的策略)
X_missing_mean = imp_mean.fit_transform(X_missing) # 训练fit + 导出predice >>>>> 特殊的接口 fit_transform
```

判断是否喊有缺失值

```python
X_missing_mean = pd.DataFrame(X_missing_mean)
X_missing_mean.isnull().sum()  # 判读是否还有缺失值 
# 布尔值 False = 0, True = 1
```

![image-20210914212631468](.\pic\image-20210914212631468.png)

```python
imp_0 = SimpleImputer(missing_values=np.nan, strategy="constant", fill_value=0) # 使用常数进行填补,用0进行填补
X_missing_0 = imp_0.fit_transform(X_missing)
pd.DataFrame(X_missing_0).info()
```

![image-20210914212702031](.\pic\image-20210914212702031.png)

**使用随机森林填补缺失值**

使用随机森林回归填补缺失值任何回归都是从特征矩阵中学习，然后求解连续型标签y的过程，之所以能够实现这个过程，是因为回归算法认为，特征矩阵和标签之前存在着某种联系。实际上，标签和特征是可以相互转换的，比如说，在一个“用地区，环境，附近学校数

量”预测“房价”的问题中，我们既可以用“地区”，“环境”，“附近学校数量”的数据来预测“房价”，也可以反过来，用“环境”，“附近学校数量”和“房价”来预测“地区”。而回归填补缺失值，正是利用了这种思想。对于一个有n个特征的数据来说，其中特征T有缺失值，我们就把特征T当作标签，其他的n-1个特征和原本的标签组成新的特征矩阵。那对于T来说，它没有缺失的部分，就是我们的Y_test，这部分数据既有标签也有特征，而它缺失的部分，只有特征没有标签，就是我们需要预测的部分。

特征T不缺失的值对应的其他n-1个特征 + 本来的标签：X_train

特征T不缺失的值：Y_train

特征T缺失的值对应的其他n-1个特征 + 本来的标签：X_test

特征T缺失的值：未知，我们需要预测的Y_test

这种做法，对于某一个特征大量缺失，其他特征却很完整的情况，非常适用。

那如果数据中除了特征T之外，其他特征也有缺失值怎么办？

答案是遍历所有的特征，**从缺失最少的开始进行填补**（因为填补缺失最少的特征所需要的准确信息最少）。

**填补一个特征时，先将其他特征的缺失值用0代替，**每完成一次回归预测，就将预测值放到原本的特征矩阵中，再继续填

补下一个特征。每一次填补完毕，有缺失值的特征会减少一个，所以每次循环后，需要用0来填补的特征就越来越少。当

进行到最后一个特征时（这个特征应该是所有特征中缺失值最多的），已经没有任何的其他特征需要用0来进行填补了，

而我们已经使用回归为其他特征填补了大量有效信息，可以用来填补缺失最多的特征。

遍历所有的特征后，数据就完整，不再有缺失值了

```python
X_missing_reg = X_missing.copy()

# 找出数据集中，缺失值从小到大排序的特征的顺序
sortindex = np.argsort(X_missing_reg.isnull().sum(axis = 0)).values # 从小到大排序的顺序所对应的索引
```

```python
X_missing_reg.isnull().sum(axis = 0)
```

![image-20210914213115954](.\pic\image-20210914213115954.png)

```python
np.argsort(X_missing_reg.isnull().sum(axis = 0))
```

![image-20210914213216156](.\pic\image-20210914213216156.png)

```python
for i in sortindex:
    #构建我们的新特征矩阵和新标签
    df = X_missing_reg
    fillc = df.iloc[:,i] # 构建新标签
    df = pd.concat([df.iloc[:,df.columns != i],pd.DataFrame(y_full)],axis=1) # 构建新特征矩阵  df.columns 获取列

    #在新特征矩阵中，对含有缺失值的列，进行0的填补
    df_0 =SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=0).fit_transform(df)
    
    #找出我们的训练集和测试集
    Ytrain = fillc[fillc.notnull()]
    Ytest = fillc[fillc.isnull()]
    Xtrain = df_0[Ytrain.index,:]
    Xtest = df_0[Ytest.index,:]
    
    #用随机森林回归来填补缺失值
    rfc = RandomForestRegressor(n_estimators=100)
    rfc = rfc.fit(Xtrain, Ytrain)
    Ypredict = rfc.predict(Xtest)
    
    #将填补好的特征返回到我们的原始的特征矩阵中
    X_missing_reg.loc[X_missing_reg.iloc[:,i].isnull(),i] = Ypredict
```

```
X_missing_reg
```

![image-20210914213416921](.\pic\image-20210914213416921.png)

```python
from sklearn.model_selection import cross_val_score
X = [X_full,X_missing_mean,X_missing_0,X_missing_reg]
mse = []
std = []
for x in X:
    estimator = RandomForestRegressor(random_state=0, n_estimators=100) # 实例化
    scores = cross_val_score(estimator,x,y_full,scoring='neg_mean_squared_error', cv=5).mean()
    mse.append(scores * -1) # 越小越好
```

```python
x_labels = ['Full data','Mean Imputation','Zero Imputation','Regressor Imputation']
colors = ['r', 'g', 'b', 'orange']
plt.figure(figsize=(12, 6))

ax = plt.subplot(111) # 添加子图

for i in np.arange(len(mse)):
    ax.barh(i, mse[i],color=colors[i], alpha=0.6, align='center') # barh 横向的条形图 y轴

ax.set_title('Imputation Techniques with Boston Data')
ax.set_xlim(left=np.min(mse) * 0.9,right=np.max(mse) * 1.1)
ax.set_yticks(np.arange(len(mse)))
ax.set_xlabel('MSE')

ax.set_yticklabels(x_labels) # 设定y的命名
plt.show()
```

![image-20210914213510460](.\pic\image-20210914213510460.png)

### 机器学习中调参的基本思想

**泛化误差**

当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，**当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大**。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。

![image-20210914214452933](.\pic\image-20210914214452933.png)

那模型的复杂度与我们的参数有什么关系呢？对树模型来说，树越茂盛，深度越深，枝叶越多，模型就越复杂。所以树模型是天生位于图的右上角的模型，随机森林是以树模型为基础，所以随机森林也是天生复杂度高的模型。随机森林的参数，都是向着一个目标去：减少模型的复杂度，把模型往图像的左边移动，防止过拟合。当然了，调参没有绝对，也有天生处于图像左边的随机森林，所以调参之前，我们要先判断，模型现在究竟处于图像的哪一边。

**模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点**

**模型太复杂就会过拟合，模型太简单就会欠拟合**

**对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂**

**树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动**

![image-20210914215002037](.\pic\image-20210914215002037.png)

### 在乳腺癌数据上的调参

```python
from sklearn.datasets import load_breast_cancer # 乳腺癌数据集
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```

```python
data = load_breast_cancer()
data.data.shape # (569, 30)
```

```
data.target
```

![image-20210914231003515](.\pic\image-20210914231003515.png)

```python
rfc = RandomForestClassifier(n_estimators=100, random_state=90)
score_pre = cross_val_score(rfc, data.data, data.target, cv = 10).mean()
score_pre
```

```
0.9648809523809524
```

```python
"""
在这里我们选择学习曲线，可以使用网格搜索吗？可以，但是只有学习曲线，才能看见趋势
我个人的倾向是，要看见n_estimators在什么取值开始变得平稳，是否一直推动模型整体准确率的上升等信息
第一次的学习曲线，可以先用来帮助我们划定范围，我们取每十个数作为一个阶段，来观察n_estimators的变化如何
引起模型整体准确率的变化
"""
#####【TIME WARNING: 30 seconds】#####
scorel = []
for i in range(0,200,10):
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90)
    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()
    scorel.append(score)
    
print(max(scorel),(scorel.index(max(scorel))*10)+1)
plt.figure(figsize=[20,5])
plt.plot(range(1,201,10),scorel)
plt.show()
```

![image-20210914231052633](.\pic\image-20210914231052633.png)

```python
scorel = []
for i in range(65,80):
    rfc = RandomForestClassifier(n_estimators=i,n_jobs=-1,random_state=90)
    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()
    scorel.append(score)
print(max(scorel),([*range(65,80)][scorel.index(max(scorel))]))
plt.figure(figsize=[20,5])
plt.plot(range(65,80),scorel)
plt.show()
```

![image-20210914231122300](.\pic\image-20210914231122300.png)

```python
"""
有一些参数是没有参照的，很难说清一个范围，这种情况下我们使用学习曲线，看趋势
从曲线跑出的结果中选取一个更小的区间，再跑曲线

开始按照参数对模型整体准确率的影响程度进行调参，首先调整max_depth
param_grid = {'n_estimators':np.arange(0, 200, 10)}
param_grid = {'max_depth':np.arange(1, 20, 1)}
    
param_grid = {'max_leaf_nodes':np.arange(25,50,1)}
对于大型数据集，可以尝试从1000来构建，先输入1000，每100个叶子一个区间，再逐渐缩小范围
有一些参数是可以找到一个范围的，或者说我们知道他们的取值和随着他们的取值，模型的整体准确率会如何变化，这样的参数我们就可以直接跑网格搜索
param_grid = {'criterion':['gini', 'entropy']}
param_grid = {'min_samples_split':np.arange(2, 2+20, 1)}
param_grid = {'min_samples_leaf':np.arange(1, 1+10, 1)}
 
param_grid = {'max_features':np.arange(5,30,1)} 
"""
```

```python
#调整max_depth
param_grid = {'max_depth':np.arange(1, 20, 1)}
# 一般根据数据的大小来进行一个试探，乳腺癌数据很小，所以可以采用1~10，或者1~20这样的试探
# 但对于像digit recognition那样的大型数据来说，我们应该尝试30~50层深度（或许还不足够
#   更应该画出学习曲线，来观察深度对模型的影响
rfc = RandomForestClassifier(n_estimators=73,random_state=90)

GS = GridSearchCV(rfc,param_grid,cv=10) # 网格搜索
GS.fit(data.data,data.target)

GS.best_score_ # 返回调整好的最佳参数对应的准确率
```

```python
0.9666353383458647
```

```python
GS.best_params_ # 显示调整出来的最佳参数
```

```python
{'max_depth': 8}
```

```python
#调整max_features
param_grid = {'max_features':np.arange(5,30,1)} 
"""
max_features是唯一一个即能够将模型往左（低方差高偏差）推，也能够将模型往右（高方差低偏差）推的参数。我
们需要根据调参前，模型所在的位置（在泛化误差最低点的左边还是右边）来决定我们要将max_features往哪边调。
现在模型位于图像左侧，我们需要的是更高的复杂度，因此我们应该把max_features往更大的方向调整，可用的特征
越多，模型才会越复杂。max_features的默认最小值是sqrt(n_features)，因此我们使用这个值作为调参范围的
最小值。
"""
rfc = RandomForestClassifier(n_estimators=39 ,random_state=90)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)

GS.best_score_
```

```python
0.968421052631579
```

```python
GS.best_params_
```

```
{'max_features': 6}
```

> > .....



## 数据预处理和特征工程

①获取数据 ②数据预处理 ③特征工程 ④建模，测试模型并预测出结果 ⑤上线，验证模型效果

#### 数据预处理Preprocessing & Impute

##### **数据无量纲化**

在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一个规格，或不同分布的数据转换到某个特定分布的需求，这种需求统称为将数据的“无量纲化”，无量纲化可以加快求解速度，也可以帮我们提升模型的精度，避免一个取值范围特别大的特征对距离计算造成影响。

数据的无量纲化可以是线性的，也可以是非线性的。线性的无量纲化包括**中心化**（Zero-centered或者Meansubtraction）处理和**缩放处理**（Scale）**中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理**。

当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到[0,1]之间，而这个过程，就叫做**数据归一化**，公式如下：
$$
x^* = x-min(x) / max(x) - min(x)
$$
在sklearn当中，我们使用**preprocessing.MinMaxScaler**来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0, 1]。

**数据归一化**

```python
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
import pandas as pd
pd.DataFrame(data)
```

![image-20210916225837311](.\pic\image-20210916225837311.png)

```python
# 实现归一化
scaler = MinMaxScaler() # 实例化
scaler = scaler.fit(data) # fit 在这里本质是生成min(x) 和 max(x)
result = scaler.transform(data) # 通过接口导出结果
result
pd.DataFrame(result)
```

![image-20210916225910579](.\pic\image-20210916225910579.png)

```python
result = scaler.fit_transform(data) # 训练和导出结果一步达成
result
```

```python
scaler.inverse_transform(result) # 将归一化后的结果逆转
```

![image-20210916230012044](.\pic\image-20210916230012044.png)

```python
# 使用MinMaxScaler的参数feature_range实现将数据归一化到[0， 1]以外的范围中
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler(feature_range=[5, 10]) # 实例化
result = scaler.fit_transform(data) # fit_transform 一步导出结果
result
```

![image-20210916230047981](.\pic\image-20210916230047981.png)

```python
# 当x中的特征数量非常多的时候，fit会报错并表示，数据量太大了计算不了，
# 此时使用pratial_fit作为训练接口
# scaler = scaler.partial_fit(data)
```

> 岳昌宏

**数据标准化**

当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做**数据标准化**(Standardization，又称Z-score normalization)，公式如下：
$$
x^* = (x-μ) / σ
$$

```python
from sklearn.preprocessing import StandardScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = StandardScaler() # 实例化
scaler.fit(data) # fit 本质是生成均值和方差
```

![image-20210917084626280](.\pic\image-20210917084626280.png)

```python
scaler.mean_ # 查看均值的属性 # array([-0.125,  9.   ])
scaler.var_ # 查看方差的属性 # array([ 0.546875, 35.      ])
```

```python
x_std = scaler.transform(data) # 通过接口导出结果
x_std
```

![image-20210917084735440](.\pic\image-20210917084735440.png)

```python
x_std.mean() # 导出的结果是一个数组，用mean()查看均值 0.0
x_std.std() # 用std()查看方差 1
```

```python
scaler.fit_transform(data) # 使用fit_transform(data)一步达成结果
```

![image-20210917084838666](.\pic\image-20210917084838666.png)

```python
scaler.inverse_transform(x_std) # 使用inverse_transform 逆转标准化
```

![image-20210917084910194](.\pic\image-20210917084910194.png)

![image-20210917085624198](.\pic\image-20210917085624198.png)

##### 缺失值

机器学习和数据挖掘中所使用的数据，永远不可能是完美的，很多字段，对于分析和建模来说意义非凡，但对于实际收集数据的人来说却不过如此，因此数据挖掘之中，常常会有很多字段缺失值很多，但又不能舍弃这些字段的情况，因此，数据预处理中非常重要的一项就是处理缺失值。

impute.Simplelmputer

![image-20210917114714021](.\pic\image-20210917114714021.png)

```python
import pandas as pd
data = pd.read_csv(r"./data/Narrativedata.csv",index_col = 0) # index_col 把第0列作为索引
data.head()
```

![image-20210917221837997](.\pic\image-20210917221837997.png)

```python
data.info()
```

![image-20210917221923173](.\pic\image-20210917221923173.png)

```python
Age = data.loc[:, "Age"].values.reshape(-1, 1) # reshape（-1， 1）将数据升维

from sklearn.impute import SimpleImputer
# 默认的缺失值是np.nan
imp_mean = SimpleImputer() # 实例化，默认用均值填补
imp_median = SimpleImputer(strategy="median") # 用中位数填补
imp_0 = SimpleImputer(strategy="constant", fill_value=0) # 用0填补	

imp_mean = imp_mean.fit_transform(Age)
imp_median = imp_median.fit_transform(Age)
imp_0 = imp_0.fit_transform(Age)
```

```python
imp_mean[:20]
```

![image-20210917222051898](.\pic\image-20210917222051898.png)

```python
imp_median[:20]
```

![image-20210917222115781](.\pic\image-20210917222115781.png)

```python
imp_0[:20]
```

![image-20210917222138006](.\pic\image-20210917222138006.png)

```
# 我们使用中位数来填补Age
data.loc[:, "Age"] = imp_median
data.info()
```

![image-20210917222842291](.\pic\image-20210917222842291.png)

```python
# 使用众数来填补Embarked
Embarked = data.loc[:, "Embarked"].values.reshape(-1, 1)
imp_mode = SimpleImputer(strategy="most_frequent")
data.loc[:, "Embarked"] = imp_mode.fit_transform(Embarked)
data.info()
```

![image-20210917222906531](.\pic\image-20210917222906531.png)



**使用Pandas和Numpy进行填补**

```python
import pandas as pd
data = pd.read_csv(r"./data/Narrativedata.csv",index_col = 0)
data.head()
data.loc[:,"Age"] = data.loc[:,"Age"].fillna(data.loc[:,"Age"].median()) # 中位数
#.fillna 在DataFrame里面直接进行填补
data.dropna(axis=0,inplace=True)
#.dropna(axis=0)删除所有有缺失值的行，.dropna(axis=1)删除所有有缺失值的列
#参数inplace，为True表示在原数据集上进行修改，为False表示生成一个复制对象，不修改原数据，默认False
```



##### 编码和哑变量

在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fifit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是["小 学"，“初中”，“高中”，"大学"]，付费方式可能包含["支付宝"，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行**编码**，即是说，**将文字型数据转换为数值型**。

**preprocessing.LabelEncoder:标签专用，能够将分类转换为分类数值**

```python
from sklearn.preprocessing import LabelEncoder
y = data.iloc[:, -1] # 要输入的是标签，不是特征矩阵，所以允许一维数据

le = LabelEncoder() # 实例化
le = le.fit(y) # 导入数据
label = le.transform(y) # transform接口调取结果

```

![image-20210918083639188](.\pic\image-20210918083639188.png)

```python
le.classes_ # 查看标签中有多少个类别
```

![image-20210918083719855](.\pic\image-20210918083719855.png)

```python
le.fit_transform(y) # 也可以直接fit_transform一步到位

le.inverse_transform(label) # 可以逆转结果

data.iloc[:, -1] = label # 让标签等于我们运行出来的结果
data.head()

from sklearn.preprocessing import LabelEncoder
data.iloc[:, -1] = LabelEncoder().fit_transform(data.iloc[:, -1])
```



**preprocessing.OrdinalEncoder:特征专用，能够将分类特征转换为分类数值**

```python
from sklearn.preprocessing import OrdinalEncoder
#接口categories_对应LabelEncoder的接口classes_，一模一样的功能
data_ = data.copy()
data_.head()
data_.info() # 不能出现缺失值
```

![image-20210918085117297](.\pic\image-20210918085117297.png)

```python
OrdinalEncoder().fit(data_.iloc[:,1:3]).categories_
```

![image-20210918085203362](.\pic\image-20210918085203362.png)

```python
data_.iloc[:,1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:,1:-1])
data_.head()
```

![image-20210918085235351](.\pic\image-20210918085235351.png)



**preprocessing.OneHotEncoder:独热编码，创建哑变量**

我们来思考三种不同性质的分类数据：

1） 舱门（S，C，Q）

三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是**名义变量。**

2） 学历（小学，初中，高中）

三种取值不是完全独立的，我们可以明显看出，在性质上可以有高中>初中>小学这样的联系，学历有高低，但是学历取值之间却不是可以计算的，我们不能说小学 + 某个取值 = 初中。这是**有序变量。**

3） 体重（>45kg，>90kg，>135kg）

各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。这是**有距变量**

![image-20210918092240074](.\pic\image-20210918092240074.png)

```
from sklearn.preprocessing import OneHotEncoder
X = data.iloc[:,1:-1]
X
```

![image-20210918092304561](.\pic\image-20210918092304561.png)

```python
enc = OneHotEncoder(categories='auto').fit(X)
result = enc.transform(X).toarray()
result
```

![image-20210918092332721](.\pic\image-20210918092332721.png)

```python
#依然可以直接一步到位，但为了给大家展示模型属性，所以还是写成了三步
OneHotEncoder(categories='auto').fit_transform(X).toarray()
#依然可以还原
pd.DataFrame(enc.inverse_transform(result))
```

```python
enc.get_feature_names() # 返回列的名字
```

![image-20210918092433857](.\pic\image-20210918092433857.png)

```python
#axis=1,表示跨行进行合并，也就是将量表左右相连，如果是axis=0，就是将量表上下相连
newdata = pd.concat([data, pd.DataFrame(result)], axis=1)
newdata.head()
```

![image-20210918092458961](.\pic\image-20210918092458961.png)

```python
newdata.drop(["Sex","Embarked"],axis=1,inplace=True)
newdata.columns = ["Age","Survived","Female","Male","Embarked_C","Embarked_Q","Embarked_S"]
newdata.head()
```

![image-20210918092521800](.\pic\image-20210918092521800.png)



![image-20210918093131388](.\pic\image-20210918093131388.png)



##### 处理连续型特征：二值化与分段

**sklearn.preprocessing.Binarizer**

根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0

```python
#将年龄二值化
data_2 = data.copy()

from sklearn.preprocessing import Binarizer
X = data_2.iloc[:,0].values.reshape(-1,1) #类为特征专用，所以不能使用一维数组
transformer = Binarizer(threshold=30).fit_transform(X)
transformer
```



**preprocessing.KBinsDiscretizer**

这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。总共包含三个重要参数:

![image-20210918094004795](.\pic\image-20210918094004795.png)

```python
from sklearn.preprocessing import KBinsDiscretizer
X = data.iloc[:,0].values.reshape(-1,1) 
est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')

est.fit_transform(X) #查看转换后分的箱：变成了一列中的三箱

set(est.fit_transform(X).ravel()) # ravel降维

est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform') #查看转换后分的箱：变成了哑变量
est.fit_transform(X).toarray()
```



#### 特征选择

![image-20210918155257775](.\pic\image-20210918155257775.png)

```python
import pandas as pd
data = pd.read_csv(r"./data/digit recognizor.csv")
data.head()

X = data.iloc[:, 1:]
y = data.iloc[:, 0]
X.shape # (42000, 784)
```

![image-20210918161315449](.\pic\image-20210918161315449.png)



**Filter过滤法**

- **方差过滤VarianceThreshold**：这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。**所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征**。

    VarianceThreshold有重要参数**threshold**，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold() #实例化，不填参数默认方差为0
X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵
#也可以直接写成 X = VairanceThreshold().fit_transform(X)
X_var0.shape # (42000, 708)

```

​		可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我		们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半		的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了：

```python
"""
X.var().values 每一列的方差
np.median(X.var().values) # 方差的中位数
"""
import numpy as np
X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)
np.median(X.var().values)
X_fsvar.shape # (42000, 392)
```

​		当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：
$$
Var[X] = p(1-p)
$$
​		其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率。

```python
#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征
X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)
X_bvar.shape # (42000, 685)
```

> > 过滤法的**主要对象**是：**需要遍历特征或升维的算法们**，而过滤法的**主要目的**是：**在维持算法表现的前提下，帮助算法们降低计算成本**。
> >
> > 从算法原理上来说，传统决策树需要遍历所有特征，计算不纯度后进行分枝，而随机森林却是随机选择特征进行计算和分枝，因此随机森林的运算更快，过滤法对随机森林无用，对决策树却有用。但是在sklearn中，决策树和随机森林都是随机选择特征进行分枝，但决策树在建模过程中随机抽取的特征数目却远远超过随机森林当中每棵树随机抽取的特征数目（比如说对于这个780维的数据，随机森林每棵树只会抽取10~20个特征，而决策树可能会抽取300~400个特征），因此，过滤法对随机森林无用，却对决策树有用



- **(相关性过滤)卡方过滤：** 卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类**feature_selection.chi2**计算每个**非负**特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合**feature_selection.SelectKBest**这个可以输入”评分准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。

    另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据）：

```python
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
#假设在这里我一直我需要300个特征
X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)
X_fschi.shape # (42000, 300)
cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean() # 0.9333098667649198
```

​	可以看出，模型的效果降低了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的K值设置得太小，要么我们需	要调整K值，要么我们必须放弃相关性过滤。当然，如果模型的表现提升，则说明我们的相关性过滤是有效的，是过滤掉了模型的噪音	的，这时候我们就保留相关性过滤的结果。

```python
#======【TIME WARNING: 5 mins】======#
%matplotlib inline
import matplotlib.pyplot as plt
score = []
for i in range(390,200,-10):
    X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)
    once = cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()
    score.append(once)
plt.plot(range(350,200,-10),score)
plt.show()
```

![image-20210918173031737](.\pic\image-20210918173031737.png)

通过这条曲线，我们可以观察到，随着K值的不断增加，模型的表现不断上升，这说明，K越大越好，数据中所有的特征都是与标签相关的

看p值选择k：

![image-20210918174456338](.\pic\image-20210918174456338.png)

从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。

```python
chivalue, pvalues_chi = chi2(X_fsvar,y) # 获取卡方值和p值
chivalue # 所有的特征对应的卡方数值
pvalues_chi # p值

#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征：
k = chivalue.shape[0] - (pvalues_chi > 0.05).sum() 
"""
chivalue.shape[0] 特征的总数
(pvalues_chi > 0.05) 是一个布尔数组
"""


#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()
```

- **(相关性过滤)F检验：**F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含**feature_selection.f_classif**（F检验分类）和**feature_selection.f_regression**（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。

    和卡方检验一样，这两个类需要和类**SelectKBest**连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会**先将数据转换成服从正态分布的方式**

    F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，**我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的**，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。

```python
from sklearn.feature_selection import f_classif
F, pvalues_f = f_classif(X_fsvar,y)
pvalues_f

k = F.shape[0] - (pvalues_f > 0.05).sum()

#X_fsF = SelectKBest(f_classif, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()
```

- **(相关性过滤)互信息法：**互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类**feature_selection.mutual_info_classif**（互信息分类）和**feature_selection.mutual_info_regression**（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系

    互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。

```python
from sklearn.feature_selection import mutual_info_classif as MIC

result = MIC(X_fsvar,y) k = result.shape[0] - sum(result <= 0)


#X_fsmic = SelectKBest(MIC, k=填写具体的k).fit_transform(X_fsvar, y)
#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsmic,y,cv=5).mean()
```

- **Embedded嵌入法：**嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。算量很大，计

    算缓慢的算法，嵌入法本身也会非常耗时耗力。并且，在选择完毕之后，我们还是需要自己来评估模型。

    **feature_selection.SelectFromModel**对于有feature_importances_的模型来说，若重要性低于提供的阈值参数，则认为这些特征不重要并被移除。feature_importances_的取值范围是[0,1]，如果设置阈值很小，比如0.001，就可以删除那些对标签预测完全没贡献的特征。如果设置得很接近1，可能只有一两个特征能够被留下

![image-20210918201431140](.\pic\image-20210918201431140.png)

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as RFC

RFC_ = RFC(n_estimators =10,random_state=0) # 随机森林

X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y) #在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征只能够分到大约0.001的feature_importances_
X_embedded.shape # (42000, 47)
#模型的维度明显被降低了


#同样的，我们也可以画学习曲线来找最佳阈值
#======【TIME WARNING：10 mins】======#
import numpy as np
import matplotlib.pyplot as plt

RFC_.fit(X,y).feature_importances_

threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20)
score = []
for i in threshold:
    X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)
    once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()
    score.append(once)
plt.plot(threshold,score)
plt.show()
```

![image-20210918202206391](.\pic\image-20210918202206391.png)



- **Wrapper包装法：**包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。

    参数**estimator**是需要填写的实例化后的评估器，**n_features_to_select**是想要选择的特征个数，**step**表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，**.support_**：返回所有的特征的是否最后被选中的布尔矩阵，以及**.ranking_**返回特征的按数次迭代中综合重要性的排名。

```python
from sklearn.feature_selection import RFE

RFC_ = RFC(n_estimators =10,random_state=0)
selector = RFE(RFC_, n_features_to_select=340, step=50).fit(X, y)

selector.support_.sum()
selector.ranking_
X_wrapper = selector.transform(X)

cross_val_score(RFC_,X_wrapper,y,cv=5).mean()

```



```python
#======【TIME WARNING: 15 mins】======#
score = []
for i in range(1,751,50):
    X_wrapper = RFE(RFC_,n_features_to_select=i, step=50).fit_transform(X,y)
    once = cross_val_score(RFC_,X_wrapper,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(1,751,50),score)
plt.xticks(range(1,751,50))
plt.show()
```

![image-20210918210947959](.\pic\image-20210918210947959.png)

## 降维算法PCA

- **维度**
    - 对于**数组和Series**来说，维度就是功能shape返回的结果，shape中返回了几个数字，就是几维。
    - 一个**特征矩阵**或一个DataFrame，这些结构永远只有一张表，所以一定有行列，其中行是样本，列是特征。针对每一张表，**维度指的是样本的数量或特征的数量，一般无特别说明，指的都是特征的数量**。除了索引之外，一个特征是一维，两个特征是二维，n个特征是n维。
    - **对图像来说，维度就是图像中特征向量的数量**，特征向量可以理解成坐标轴
- 降维算法中的**降维**：指的是降低特征矩阵中特征的数量，其实还有另外一种需求，就是**数据可视化**
- **PCA和特征选择的区别**：特征选择是从已经存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原始数据的那个位置，代表原始数据的什么含义。而PCA，是将以存在的数据进行压缩，**降维完毕的特征不是原本特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征**。通常来说：在新的特征生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原始数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原始数据上代表的含义了，以PCA为代表的降维算法因此是**特征创造**的一种

#### PCA

在降维中，**PCA使用的信息衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息也就越多**

![image-20210919100458757](.\pic\image-20210919100458757.png)

Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。

```python
class sklearn.decomposition.PCA (n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0,
iterated_power=’auto’, random_state=None)
```

![image-20210919100811604](.\pic\image-20210919100811604.png)

![image-20210919100946121](.\pic\image-20210919100946121.png)

每个特征的数据一模一样，因此方差也都为1，数据的方差总和是2。

![image-20210919101053048](.\pic\image-20210919101053048.png)

![image-20210919102137970](.\pic\image-20210919102137970.png)

**重要参数**

```python
class sklearn.decomposition.PCA (n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0,
iterated_power=’auto’, random_state=None)
```

**n_components**:是降维后需要的维度，即降维后需要保留的特征数量，这是一个需要我们去确认的超参数，并且我们设定的数字会影响到模型的表现，如果特征太多，就达不到降维的效果，如果留下的特征太少，那新特征向量可能无法容纳原始数据集中大部分信息，因此n_components不能太大也不能太小。如果我们希望可视化一组数据来观察数据的分布，我们往往将数据降到三维一下，很多是时候是二维，即n_components的取值是2

```python
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris # 鸢尾花数据集
from sklearn.decomposition import PCA

iris = load_iris() # 导入数据
X = iris.data # 特征
y = iris.target # 标签

X.shape # 作为数据 是二维 (150, 4)

import pandas as pd
pd.DataFrame(X) # 作为数据表或者特征矩阵是 四维
```

![image-20210920171110985](.\pic\image-20210920171110985.png)

```python
# 四维数据是无法画图的，我们将数据降低到二维来画图
pca = PCA(n_components = 2) # 实例化
pca = pca.fit(X) # 拟合模型
X_dr = pca.transform(X) # 获取新的矩阵
X_dr

# 也可以一步到位
# X_dr = PCA(2).fit_transform(X)
pd.DataFrame(X_dr)
```

![image-20210920171205014](.\pic\image-20210920171205014.png)

```python
# 可视化
plt.figure() # 现在我要画图了， 床架一个画布
# X_dr[y == 0, 0], X_dr[y == 0, 1] # 取出y==0的行，的第一列 和 第二列
plt.scatter(X_dr[y == 0, 0], X_dr[y == 0, 1], c="r", label = iris.target_names[0])
plt.scatter(X_dr[y == 1, 0], X_dr[y == 1, 1], c="b", label = iris.target_names[1])
plt.scatter(X_dr[y == 2, 0], X_dr[y == 2, 1], c="orange", label = iris.target_names[2])

plt.legend()
plt.title("PCA of IRIS dataset")
plt.show()

'''
colors = ['red', 'black', 'orange']
iris.target_names
plt.figure()
for i in [0, 1, 2]:
    plt.scatter(X_dr[y == i, 0],X_dr[y == i, 1], alpha=.7, c=colors[i], label=iris.target_names[i] )
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()


#  alpha=.7 画出图像的透明度
'''
```

![image-20210920191851076](.\pic\image-20210920191851076.png)

**pca重要属性**

```python
# 属性explained_variance_,查看降维后每个新特征向量上所带的信息量大小(可解释性方差的大小)
pca.explained_variance_  # array([4.22824171, 0.24267075])
```

```python
# 属性explained_variance_ratio_，查看降维后每个新特征向量所占的信息量占原始数据总量的百分比
# 又叫做可解释方差贡献率
pca.explained_variance_ratio_  # array([0.92461872, 0.05306648])
# 发现大部分信息都被有效集中到第一个特征上
pca.explained_variance_ratio_.sum() # 0.9776852063187949
```



当参数**n_components**中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于**转换了新特征空间**，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出**累计可解释方差贡献率曲线**，以此选择最好的n_components的整数取值。

**累积可解释方差贡献率曲线**是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值

例如：

```python
pca_line = PCA().fit(X)
pca_line.explained_variance_ratio_  # array([0.92461872, 0.05306648, 0.01710261, 0.00521218])
pca_line.explained_variance_ratio_.sum() # 1

import numpy as np
np.cumsum(pca_line.explained_variance_ratio_) # array([0.92461872, 0.97768521, 0.99478782, 1.        ])
```

```python
import numpy as np
pca_line = PCA().fit(X)
plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))
plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数
plt.xlabel("number of components after dimension reduction")
plt.ylabel("cumulative explained variance ratio")
plt.show()
```

![image-20210920194444394](.\pic\image-20210920194444394.png)



**n_components最大似然估计自选超参数mle**

```python
pca_mle = PCA(n_components="mle")
pca_mle = pca_mle.fit(X)
X_mle = pca_mle.transform(X)
X_mle
#可以发现，mle为我们自动选择了3个特征
pca_mle.explained_variance_ratio_.sum()  # 0.9947878161267246
#得到了比设定2个特征时更高的信息含量，对于鸢尾花这个很小的数据集来说，3个特征对应这么高的信息含量，并不需要去纠结于只保留2个特征，毕竟三个特征也可以可视化

# 但是计算量会很大
```

**n_components按信息量占比选超参数**

输入[0,1]之间的浮点数，并且让参数svd_solver =='full'，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。比如说，如果我们希望保留97%的信息量，就可以输入n_components = 0.97，PCA会自动选出能够让保留的信息量超过97%的特征数量。

```python
pca_f = PCA(n_components=0.97,svd_solver="full")
pca_f = pca_f.fit(X)
X_f = pca_f.transform(X)
pca_f.explained_variance_ratio_ # array([0.92461872, 0.05306648])

```

有四种模式可选："auto", "full", "arpack",和"randomized"，默认”auto"。

- "auto":基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生.
- "full"：**适合数据量比较适中，计算时间充足的情况**
- "arpark"：，**可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况**
- "randomized"：**适合特征矩阵巨大，计算量庞大的情况**



> > 在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵



**pca重要属性components_**

```python
PCA(2).fit(X).components_ # 降维过后的新特征空间V(k, n) k是需要的特征数，n是原始的特征数
```

![image-20210920223001733](.\pic\image-20210920223001733.png)

```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

faces = fetch_lfw_people(min_faces_per_person=60) # 实例化

faces.data.shape # (1348, 2914)
# 行是样本
# 列是样本相关的所有特征

faces.images.shape  # (1348, 62, 47)
# 1348 是矩阵中图像的个数
# 62 是每个图像的特征矩阵的行
# 47 是每个图像的特征矩阵的列

#创建画布和子图对象
fig, axes = plt.subplots(3, 8, figsize=(8,4), subplot_kw = {"xticks":[],"yticks":[]} )#不要显示坐标轴
for i, ax in enumerate(axes.flat):
    ax.imshow(faces.images[i,:,:], cmap="gray") #选择色彩的模式 
```

![image-20210921104054250](.\pic\image-20210921104054250.png)

```python
#原本有2900维，我们现在来降到150维
pca = PCA(150).fit(X) 
V = pca.components_ # 用来映射的新向量特征空间
V.shape #(150, 2914)

fig, axes = plt.subplots(3,8,figsize=(8,4),subplot_kw = {"xticks":[],"yticks":[]})
for i, ax in enumerate(axes.flat):
    ax.imshow(V[i,:].reshape(62,47),cmap="gray")
```

![image-20210921104149751](.\pic\image-20210921104149751.png)



**重要接口inverse_transform**

```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

faces = fetch_lfw_people(min_faces_per_person=60)
X = faces.data

# 降维
pca = PCA(150)
X_dr = pca.fit_transform(X)
X_dr.shape # (1348, 150)

# 逆转
X_inverse = pca.inverse_transform(X_dr) # 逆转
X_inverse.shape # (1348, 2914)

# 画图
fig, ax = plt.subplots(2,10,figsize=(10,2.5),subplot_kw={"xticks":[],"yticks":[]} )
for i in range(10):
    ax[0,i].imshow(faces.images[i,:,:],cmap="binary_r")
    ax[1,i].imshow(X_inverse[i].reshape(62,47),cmap="binary_r")
    
```

![image-20210921105145889](.\pic\image-20210921105145889.png)

可以明显看出，这两组数据可视化后，由降维后再通过inverse_transform转换回原维度的数据画出的图像和原数据画的图像大致相似，但原数据的图像明显更加清晰。这说明，inverse_transform并没有实现数据的完全逆转。这是因为，在降维的时候，部分信息已经被舍弃了，X_dr中往往不会包含原数据100%的信息，所以在逆转的时候，即便维度升高，原数据中已经被舍弃的信息也不可能再回来了。所以，**降维不是完全可逆的**。



**用PCA做噪音过滤**

降维的目的之一就是希望抛弃掉对模型带来负面影响的特征，而我们相信，带有效信息的特征的方差应该是远大于噪音的，所以相比噪音，有效的特征所带的信息应该不会在PCA过程中被大量抛弃。inverse_transform能够在不恢复原始数据的情况下，将降维后的数据返回到原本的高维空间，即是说能够实现”保证维度，但去掉方差很小特征所带的信息“。利用inverse_transform的这个性质，我们能够实现噪音过滤。

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

digits = load_digits()
digits.data.shape # (1797, 64)

digits.target # array([0, 1, 2, ..., 8, 9, 8])
set(digits.target.tolist()) # {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
digits.data
```

![image-20210921154139561](.\pic\image-20210921154139561.png)

```python
digits.images.shape # (1797, 8, 8)

def plot_digits(data):
    fig, axes = plt.subplots(4,10,figsize=(10,4),subplot_kw = {"xticks":[],"yticks":[]})
    for i, ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8,8),cmap="binary")
        
plot_digits(digits.data)
```

![image-20210921155123610](.\pic\image-20210921155123610.png)

```python
np.random.RandomState(42)
#在指定的数据集中，随机抽取服从正态分布的数据
#两个参数，分别是指定的数据集，和抽取出来的正太分布的方差
noisy = np.random.normal(digits.data,2) # 方差越大，随机抽取的数据就越乱
plot_digits(noisy)
```

![image-20210921155532839](.\pic\image-20210921155532839.png)

```python
pca = PCA(0.5).fit(noisy)
X_dr = pca.transform(noisy)
X_dr.shape # (1797, 6)

without_noise = pca.inverse_transform(X_dr)
plot_digits(without_noise)

```

![image-20210921155615655](.\pic\image-20210921155615655.png)

